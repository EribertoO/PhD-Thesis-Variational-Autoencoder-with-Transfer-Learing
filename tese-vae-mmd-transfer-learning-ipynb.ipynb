{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Autor:** [Eriberto Oliveira do Nascimento](https://englishphonetics.net/about-us.html)<br>\n**Data de criação:** 17/02/2019<br>\n**Última modificação:** 07/05/2023<br>\n**Descrição:** Rede Neural Convolutional Variacional AutoCodificadora (CVAE) e Transferência de Aprendizagem Profunda com função de perda personalizada. <br>\n**Descrição Longa:** Esse é o código desenvolvimento para a tese de doutorado - DESENVOLVIMENTO DA TRANSFERÊNCIA DE APRENDIZAGEM VIA REDES\nNEURAIS ARTIFICIAIS PROFUNDAS NA MODELAGEM DO ÍNDICE DE\nTRANSMISSÃO DA FALA <br>","metadata":{"_uuid":"bd995939-87e4-495a-acef-90f6a4f2ed3d","_cell_guid":"9db3e0b3-ca7f-42b2-8c3e-3b3b032b3b31","trusted":true}},{"cell_type":"markdown","source":"**Author:** [Eriberto Oliveira do Nascimento](https://englishphonetics.net/about-us.html)<br>\n**Date created:** 2019/17/02<br>\n**Last modified:** 2023/05/01<br>\n**Description:** Convolutional Variational AutoEncoder (CVAE) and Deep Transfer Learning with a custom loss function. <br>\n**Long Description:** This is the development code for the doctoral thesis - DEVELOPMENT OF DEEP TRANSFER TRANSFER LEARNING VIA ARTIFICIAL NEURAL NETWORKS IN MODELING THE INDEX OF\nSPEECH TRANSMISSION. <br>","metadata":{"_uuid":"9879918b-e5ce-4cc7-b926-222b68f2106e","_cell_guid":"8d5f777f-b475-4c44-bb05-612e09f09ef9","trusted":true}},{"cell_type":"code","source":"\"\"\"### 1 - Setup\"\"\"\n#from __future__ import print_function\n# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\nfrom keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\nfrom keras.utils import np_utils\n\n# prerequisites for the CVAE \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import objectives\nfrom scipy.stats import norm\n\nimport pandas as pd\n!pip install acoustics","metadata":{"_uuid":"01ed76ba-a525-4029-8059-e57c6ba8ed54","_cell_guid":"dc5a8b0a-d85d-4685-9cb4-c70ac4f14fe2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:24.112403Z","iopub.execute_input":"2023-05-07T19:10:24.112796Z","iopub.status.idle":"2023-05-07T19:10:43.239620Z","shell.execute_reply.started":"2023-05-07T19:10:24.112712Z","shell.execute_reply":"2023-05-07T19:10:43.238656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1 - Geração sintética da Resposta Impulsiva de Sala\n### 1 - Synthetic room impulsive (RIR) response generation","metadata":{"_uuid":"91027cda-98ce-4c65-998b-5984b0af93d1","_cell_guid":"f5607e7d-226d-4a1f-9c19-d9168c053fdc","trusted":true}},{"cell_type":"code","source":"# !pip install pyroomacoustics==0.4.1\n# !pip install matplotlib==3.2.2","metadata":{"_uuid":"b3fd5979-bdb1-400f-bd02-66118095a493","_cell_guid":"76c5bf02-9cdf-4e34-b92f-f6c0a1ebeff1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:43.243015Z","iopub.execute_input":"2023-05-07T19:10:43.243326Z","iopub.status.idle":"2023-05-07T19:10:43.248880Z","shell.execute_reply.started":"2023-05-07T19:10:43.243299Z","shell.execute_reply":"2023-05-07T19:10:43.247955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThis example creates a room with reverberation time specified by inverting Sabine's formula.\nThis results in a reverberation time slightly longer than desired.\nThe simulation is pure image source method.\n\"\"\"\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.io import wavfile\nimport librosa\n# Uncoment this module to run it localy\n# import pyroomacoustics as pra\nimport random\nimport time\n\ndef RIR_generator(signal_excitation):\n    \n    methods = [\"ism\", \"hybrid\"]\n\n    if __name__ == \"__main__\":\n\n        parser = argparse.ArgumentParser(\n            description=\"Simulates and adds reverberation to a dry sound sample.\"\n        )\n        parser.add_argument(\n            \"--method\",\n            \"-m\",\n            choices=methods,\n            default=methods[1],\n            help=\"Simulation method to use\",\n        )\n        \n        args = parser.parse_args()\n\n        # The desired reverberation time and dimensions of the room\n        rt60_tgt = 2*random.random() + 0.1; # [seconds]\n        rt60_tgt =round(rt60_tgt,2)\n        room_dim = random.sample(range(5, 20), 3); # [meters]\n        room_dim[2] = int(np.array(random.sample(range(2, 5), 1)))\n        \n        print(room_dim)\n\n        # import a mono wavfile as the source signal\n        # the sampling frequency should match that of the room\n        fs, audio = wavfile.read(signal_excitation)\n\n        # We invert Sabine's formula to obtain the parameters for the ISM simulator\n        e_absorption, max_order = pra.inverse_sabine(rt60_tgt, room_dim)\n\n        # Create the room\n        if args.method == \"ism\":\n            room = pra.ShoeBox(\n                room_dim, fs=fs, materials=pra.Material(e_absorption), max_order=max_order\n            )\n        elif args.method == \"hybrid\":\n            room = pra.ShoeBox(\n                room_dim,\n                fs=fs,\n                materials=pra.Material(e_absorption),\n                max_order=3,\n                ray_tracing=True,\n                air_absorption=True,\n            )\n\n        fig, ax = room.plot()\n\n        # place the source in the room\n        source_pos = np.array(room_dim)/2;\n        source_pos[2] = 1.5\n        source_pos = source_pos.tolist()\n\n        # print(source_pos)\n        \n        # source_pos = [2,2,1.5]\n        room.add_source(source_pos, signal=audio, delay=0.5)\n\n        # define the locations of the microphones\n        \n        mic1 =   np.round( np.array(source_pos )+ np.array( [source_pos[0]*0.4, 0, 0]), 2)\n        # mic2 =   np.round(np.array(source_pos )+ np.array( [-source_pos[0]/2.5, 0, 0]), 2)\n        # mic3 =   np.round( np.array(source_pos )+ np.array( [0, source_pos[1]/2.5, 0]), 2)\n        mic4 =   np.round( np.array(source_pos )+ np.array( [0, -source_pos[1]/2.5, 0]), 2 )\n\n        mic1 = mic1.tolist()\n        # mic2 = mic2.tolist()\n        # mic3 = mic3.tolist()\n        # mic4 = mic4.tolist()\n\n        fig, ax = room.plot()\n        ax.set_xlim([-1, 10])\n        ax.set_ylim([-1, 10]);\n\n        \n        mic_locs = np.c_[\n            mic1, mic4, \n        ]\n        \n        \"\"\"\n         mic_locs = np.c_[\n              [1,1,0.5], [1, 0.5, 2],  # mic 1  # mic 2\n          ]\n        \"\"\"\n\n        # finally place the array in the room\n        room.add_microphone_array(mic_locs)\n\n        # Run the simulation (this will also build the RIR automatically)\n        room.simulate()\n        \n        \"\"\"\n          room.mic_array.to_wav(\n              'Excitacao23.wav',\n              norm=True,\n             bitdepth=np.int16,\n          )\n        \"\"\"\n        # measure the reverberation time\n        rt60 = room.measure_rt60()\n        print(\"The desired RT60 was {}\".format(rt60_tgt))\n        print(\"The measured RT60 is {}\".format(rt60[1, 0]))\n\n        #Create a plot\n        plt.figure()\n\n        # plot one of the RIR. both can also be plotted using room.plot_rir()\n        rir_1_0 = room.rir[1][0]\n        rir_2_0 = room.rir[0][0]\n        \n        plt.subplot(2, 1, 1)\n        plt.plot(np.arange(len(rir_1_0)) / room.fs, rir_1_0)\n        plt.title(\"The RIR from source 0 to mic 1\")\n        plt.xlabel(\"Time [s]\")\n\n        # plot signal at microphone 1\n        plt.subplot(2, 1, 2)\n        plt.plot(room.mic_array.signals[1, :])\n        plt.title(\"Microphone 1 signal\")\n        plt.xlabel(\"Time [s]\")\n\n        plt.tight_layout()\n        plt.show()\n        \n        librosa.output.write_wav('Pyroom_mic1_Dim' + str(room_dim) + '_RT_' + str(round(rt60_tgt,2)) + '_seg' +str(int(round(time.time(),0)))+'.wav', room.rir[1][0], room.fs, norm=False)\n        librosa.output.write_wav('Pyroom_mic2_Dim' + str(room_dim) + '_RT_' + str(round(rt60_tgt,2)) + '_seg' + str(int(round(time.time(),0)))+'.wav', room.rir[0][0], room.fs, norm=False)\n\ndef generate_RIR():\n    i = 0;\n    # i represents the total numbers of RIR generated\n    while (i < 2):\n        try:\n                RIR_generator('Excitacao.wav')\n        except Exception:\n            # In any case of error during the function execution\n            pass\n\n        i = i +1\n        print(i)\n        \n# Uncoment to run the generate_RIR() method that \n# generate the RIR curves\n# generate()","metadata":{"_uuid":"1916da3b-e200-4f8c-a429-0a9833b31d70","_cell_guid":"ba1f6dfa-5f28-444d-8edf-5733a5bce210","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:43.251194Z","iopub.execute_input":"2023-05-07T19:10:43.251555Z","iopub.status.idle":"2023-05-07T19:10:44.571288Z","shell.execute_reply.started":"2023-05-07T19:10:43.251521Z","shell.execute_reply":"2023-05-07T19:10:44.570248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Geração das amostras de ruído de fundo (BGN)\n### 2 - Background noise (BGN) samples generation","metadata":{"_uuid":"26459f5b-6bea-4b23-985e-2bf66eb06179","_cell_guid":"d52da7db-cf54-4028-b7ee-95c301b9be13","trusted":true}},{"cell_type":"code","source":"# This is the link for all background noise sample applied on this thesis","metadata":{"_uuid":"02b71123-0067-4626-ac10-dde1194a641e","_cell_guid":"ee0c995d-38d2-4a74-b034-f1f6f5845cd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:44.572849Z","iopub.execute_input":"2023-05-07T19:10:44.573194Z","iopub.status.idle":"2023-05-07T19:10:44.578781Z","shell.execute_reply.started":"2023-05-07T19:10:44.573156Z","shell.execute_reply":"2023-05-07T19:10:44.577938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Cálculos de Densidade Espectral de Potência\n### 3 - Calculations of Power Spectral Density (PSD)","metadata":{"_uuid":"da2bb2cf-99df-49ec-989e-30c8d3182032","_cell_guid":"d762a0ef-e564-4469-ada0-f8a3b1cc7f8a","trusted":true}},{"cell_type":"code","source":"# !pip install acoustics","metadata":{"_uuid":"3d2b2bbd-0deb-4c3f-a8ac-6c4160e7c32c","_cell_guid":"3d833e70-c75b-4e8f-b75c-9665f9f9bf3f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:44.582335Z","iopub.execute_input":"2023-05-07T19:10:44.582800Z","iopub.status.idle":"2023-05-07T19:10:44.587908Z","shell.execute_reply.started":"2023-05-07T19:10:44.582760Z","shell.execute_reply":"2023-05-07T19:10:44.587149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.io import wavfile\nimport  scipy \nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import signal\nimport librosa\nfrom scipy.fft import fft\nfrom acoustics import Signal\nimport acoustics\n\n\"\"\"\nThis example creates a room with reverberation time specified by inverting Sabine's formula.\nThis results in a reverberation time slightly longer than desired.\nThe simulation is pure image source method.\n\"\"\"\n    \nfile_name = \"/kaggle/input/sti-prediction/RVB2014_type1_rir_largeroom1_far_angla.wav\"\nsig, sr = librosa.load(file_name)\n\nfreqs, psd = signal.welch(sig,sr)\n\nplt.figure(figsize=(5, 4))\nplt.semilogx(freqs, psd)\nplt.title('PSD: power spectral density')\nplt.xlabel('Frequency')\nplt.ylabel('Power')\nplt.tight_layout()\nplt.show()\n\nN = len(sig)\nT = 1 / sr\nyf = fft(sig)\nxf = np.linspace(0.0, 1.0/(2.0*T), N//2)\nplt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\nplt.grid()\nplt.show()\n\n# Acustica\n# RIR\ns = Signal.from_wav(file_name)\ns.fs\ns.channels\ns.samples\n\ns.plot_power_spectrum()\ns.spectrogram()\ns.plot_levels()\ns.plot_octaves()\nfig = s.plot_third_octaves()\n\n# Nova tr\nbands = acoustics.bands.octave(125, 2000)\nb = acoustics.room.t60_impulse(file_name, bands, rt='t30')\nprint(\"Tempo de reverberacao\")\nprint(b)\nplt.show()\n\n# Ruido\nprint('Adicao do ruido')\n\nfilename_BGN = \"/kaggle/input/sti-prediction/noise-free-sound-0042.wav\"\ns2 = Signal.from_wav(filename_BGN)\nBGN_octave = acoustics.signal.octaves(s2, \n                                      s2.fs, \n                                      density=False, \n                                      frequencies=[ 63., \n                                                   125., \n                                                   250., \n                                                   500., \n                                                   1000., \n                                                   2000], \n                                      ref=2e-05)\n\nRIR_level  = Signal.from_wav(file_name)\n\nSpeech_level = acoustics.signal.octaves(RIR_level, \n                                        RIR_level.fs, \n                                        density=False, \n                                        frequencies=[63., \n                                                     125., \n                                                     250., \n                                                     500., \n                                                     1000., \n                                                     2000], \n                                        ref=2e-05)\n\nprint('Ruído de fundo')\nprint(BGN_octave)\n\nprint('Nivel operacional da Fala')\nprint(Speech_level)\n\nprint(\"SNR\")\nSNR = Speech_level[1] - BGN_octave[1]\nprint(SNR)","metadata":{"_uuid":"d401f7f9-3cec-492e-a7e8-78a5a8e2da1c","_cell_guid":"54e3f976-a83a-4ba8-9f5e-0c167884dda5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:44.590995Z","iopub.execute_input":"2023-05-07T19:10:44.591367Z","iopub.status.idle":"2023-05-07T19:10:51.365328Z","shell.execute_reply.started":"2023-05-07T19:10:44.591332Z","shell.execute_reply":"2023-05-07T19:10:51.364378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 - Criação do Conjunto de Dados STI (Domínios de Origem e Alvo)\n### 4 - Creation of the STI (Source-Target Domains) Dataset","metadata":{"_uuid":"4efe1db5-97f6-499e-95ed-2cbfca40c638","_cell_guid":"2e285383-c0b9-4928-8ea9-0602cbc4a467","trusted":true}},{"cell_type":"code","source":"import os\nimport glob\nimport acoustics\nfrom acoustics import Signal\nimport numpy as np\nimport math\nimport librosa\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd\nimport os\nfrom scipy import signal\nfrom numpy import savez_compressed\n\n\"\"\"\n- Simulated STI values calculated using Shoreder Equation\n\"\"\"\n\ndef BGN_audios(folder):\n   \n    \"\"\"\n    Recursively converts WAV to spetogram arrays\n    folder - folder to convert.\n    \"\"\"\n\n    allFiles = []\n    \n    for root, dirs, files in os.walk(folder):\n        allFiles += [os.path.join(root, f) for f in files\n                     if f.endswith('.wav')]\n\n    return allFiles\n\ndef RIR_audios(folder):\n   \n    \"\"\"\n    Recursively converts WAV to spetogram arrays\n    folder - folder to convert.\n    \"\"\"\n    \n    allFiles = []\n    \n    for root, dirs, files in os.walk(folder):\n        allFiles += [os.path.join(root, f) for f in files\n                     if f.endswith('.wav')]\n\n    return allFiles\n        \ndef RIR_BGN_2_STI(RIR_file, BGN_file, id_number):\n\n    sig_BGN, sr_BGN = librosa.load(BGN_file)\n    freqs_BGN, psd_BGN = signal.welch(sig_BGN,sr_BGN)\n        \n    sig_RIR, sr_RIR = librosa.load(RIR_file)\n    freqs_RIR, psd_RIR = signal.welch(sig_RIR, sr_RIR)\n\n    # Assegura que avalia frequencias menores que 8000\n\n    psd_BGN = psd_BGN[0:len(freqs_BGN[freqs_BGN < 8000])]\n    psd_RIR = psd_RIR[0:len(freqs_RIR[freqs_RIR< 8000])]\n\n    # Interpola\n    xvals = np.arange(0,8001,10)\n    psd_BGN = np.interp(xvals, freqs_BGN[freqs_BGN < 8000], psd_BGN)\n    psd_RIR = np.interp(xvals, freqs_RIR[freqs_RIR< 8000], psd_RIR)\n\n    # Normaliza a potencia\n    psd_BGN = 10 * np.log10(psd_BGN)\n    psd_RIR = 10 * np.log10(psd_RIR)\n    \n\n    RT_octave = acoustics.room.t60_impulse(RIR_file, \n                                           acoustics.bands.octave(125, 8000),\n                                           rt='t30')\n\n    dummy_BGN = Signal.from_wav(BGN_file)\n    BGN_octave = acoustics.signal.octaves(dummy_BGN, \n                                          dummy_BGN.fs,\n                                          density=False,  \n                                          frequencies=acoustics.bands.octave(125, 8000), \n                                          ref=2e-05)\n    \n    Ln = BGN_octave[1]\n    Op_SL_octave = [61.4, 65.6, 62.3, 56.8, 51.3, 42.6, 33.6] # conforme a norma\n\n    # s =  os.path.basename(RIR_file)\n    # start = s.find(\"[\") + len(\"[\"); end = s.find(\"]\"); substring = s[start:end]\n    # chunks = substring.split(',')\n    # V = int(chunks[0]) * int(chunks[1]) * int(chunks[2])\n    # q = 2;\n\n    # Classroom noise-to-signal\n    # Classroom_SNR = 0.0032*V*q*(1/RT_octave)* np.exp(0.16/RT_octave) * \n    #  10**( 0.1*(Ln - Op_SL_octave) )\n    \n    modulation_freq = np.array([0.63,\n                                0.8, \n                                1.0, \n                                1.25, \n                                1.6, \n                                2.0, \n                                2.5, \n                                3.15,\n                                5.0, \n                                6.3, \n                                8.0, \n                                10, \n                                12.5])\n    \n    octave_freq = np.array([125, 250, 500, 1000, 2000, 4000, 8000])\n    \n    # Inicialization\n    \n    mTF = np.zeros((len(modulation_freq), len(octave_freq)))\n    SNR = np.zeros((len(modulation_freq), len(octave_freq)))\n    TI = np.zeros((len(modulation_freq), len(octave_freq)))\n    MTI = np.zeros(len(octave_freq))\n    \n    # weighing\n    alphas_Males = np.array([0.085, 0.127, 0.230, 0.233, 0.309, 0.224, 0.173])\n    betas_Males =  np.array([0.085, 0.078, 0.065, 0.011, 0.047, 0.095])\n   \n    for m in range(0,mTF.shape[0]):\n        \n        for octave in range(0,mTF.shape[1]):\n\n            mTF[m,octave] = (( 1 + (2*math.pi*modulation_freq[m]*RT_octave[octave]/ 13.82)**2 )**\n                             (-1/2))*(1 + 10**( -(Op_SL_octave[octave]  - Ln[octave])/10 ))**(-1)\n\n           # mTF[m,octave] = (( 1 + (2*math.pi*modulation_freq[m]*RT_octave[octave]/ 13.82)**2 )**(-1/2))* (1 +  Classroom_SNR[octave])**(-1)\n            SNR[m,octave] = 10 * np.log10( mTF[m,octave] / (1 - mTF[m,octave] ))\n\n            if SNR[m,octave] > 15:\n                 SNR[m,octave] = 15\n            if SNR[m,octave] < -15:\n                 SNR[m,octave] = -15 \n\n            TI[m,octave] = (SNR[m,octave] + 15 ) / 30\n            \n            MTI[octave] = ( 1 / len(modulation_freq) ) * sum( TI[:,octave])\n\n            if MTI[octave] > 1:\n                 MTI[octave] = 1\n\n            STI = np.dot(alphas_Males,MTI)\n\n    #  return [id_number, np.round(STI,3), np.round(RT_octave,3), np.round( BGN_octave[1],3)]\n    room_id = id_number  * np.ones(( len(xvals ), 1) )\n    room_STI = np.round(STI,3) * np.ones(( len(xvals ), 1) )\n    \n    \"\"\"\n      df = pd.DataFrame({'Room_id': room_id, \n                         'STI': room_STI,\n                         'Frequ': np.transpose(xvals),\n                         'PSD_RIR':  psd_RIR,\n                         'PSD_BGN': psd_BGN\n                         },\n                        index = xvals\n                        )\n    \"\"\"\n    output = np.concatenate([room_id, room_STI, \n                             xvals.reshape(len(xvals),1),\n                             psd_RIR.reshape(len(psd_RIR),1), \n                             psd_BGN.reshape(len(psd_BGN),1) ] , 1)\n\n    return  output\n\nfolder_RIR = r\"RIR_simuladas\"\nfolder_BGN = r\"Background_Noise\"\n\n# print(\"Number of simulated RIR .wav files\")\n# print(RIR_audios(folder_RIR))\n# print(\"Number of meausured BGB .wav files\")\n# print(BGN_audios(folder_BGN))\n\nBGN = BGN_audios(folder_BGN)\nRIR = RIR_audios(folder_RIR)\n\n# print( RIR_BGN_2_STI(RIR[10], BGN[6], 5) )\n\n# Teste gerar STFT bacgroud\n# audio2_STFT_spectogram(BGN[6],5)\n\n# Teste CWT \n# audio2_CWT_spectogram(RIR[20],5)\n\n# Cria amostras\n\nid_number = 0\ndummy_RIR = random.randint(0,10500)\ndummy_BGN = random.randint(0,686)\n\ndef generate_STI_database():\n    \n    A = RIR_BGN_2_STI(RIR[dummy_RIR], BGN[dummy_BGN], id_number)\n\n    # print(A)\n    # print(A.shape)\n\n    id_number = 1\n    while id_number < 30001:\n\n        dummy_RIR = random.randint(0,8)\n        dummy_BGN = random.sample(range(1, 686), 5)\n\n        for i in dummy_BGN:\n            A = np.vstack([A, RIR_BGN_2_STI(RIR[dummy_RIR], BGN[i], id_number )])\n            id_number =  id_number + 1\n            # pd.DataFrame(A).to_csv(\"Train_Database_identification.csv\")\n        print(id_number)\n\n    dataset = pd.DataFrame({'c1': A[:,0], 'c2': A[:,1], \n                            'c3': A[:,2], 'c4': A[:,3], \n                            'c5': A[:,4]})\n\n    # dataset.to_pickle(\"./dummy.pkl\")\n    # savez_compressed('data_compres.npz', A)\n    # unpickled_df = pd.read_pickle(\"./dummy.pkl\")","metadata":{"_uuid":"7b2a33d5-c4fe-4426-a974-699d5ebcce1e","_cell_guid":"6ad179ea-805b-4790-95e3-d717f763d744","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:51.366950Z","iopub.execute_input":"2023-05-07T19:10:51.367349Z","iopub.status.idle":"2023-05-07T19:10:51.395578Z","shell.execute_reply.started":"2023-05-07T19:10:51.367313Z","shell.execute_reply":"2023-05-07T19:10:51.394693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 - Treinamento de Autoencoder Variacional (VAE) no Domínio de Origem.\n### 5 - Variational Autoencoder (VAE) Training for the Source Domain.","metadata":{"_uuid":"148f0843-144a-428c-a536-365b424cca04","_cell_guid":"baccc300-e44d-41fc-83c1-600566508009","trusted":true}},{"cell_type":"code","source":"# Load raw data\nA = np.load('../input/sti-prediction/data_compres.npz')\nA.files\nA = A['arr_0']\n\ndf = pd.DataFrame({'c1': A[:,0], 'c2': A[:,1], \n                   'c3': A[:,2], 'c4': A[:,3], \n                   'c5': A[:,4]})\n\n\"\"\"## 2 - Functions settings\n\"\"\"\n\ndef feature_normalize(dataset):\n\n    mu = np.mean(dataset, axis=0)\n    sigma = np.std(dataset, axis=0)\n    return (dataset - mu)/sigma\n\n\ndef show_confusion_matrix(validations, predictions):\n\n    matrix = metrics.confusion_matrix(validations, predictions)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(matrix,\n                cmap=\"coolwarm\",\n                linecolor='white',\n                linewidths=1,\n                xticklabels=LABELS,\n                yticklabels=LABELS,\n                annot=True,\n                fmt=\"d\")\n    \n    plt.title(\"Confusion Matrix\")\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.show()\n\n\ndef show_basic_dataframe_info(dataframe, preview_rows=20):\n\n    \"\"\"\n    This function shows basic information for the given dataframe\n    Args:\n        dataframe: A Pandas DataFrame expected to contain data\n        preview_rows: An integer value of how many rows to preview\n    Returns:\n        Nothing\n    \"\"\"\n\n    # Shape and how many rows and columns\n    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n    print(\"First 20 rows of the dataframe:\\n\")\n    # Show first 20 rows\n    print(dataframe.head(preview_rows))\n    print(\"\\nDescription of dataframe:\\n\")\n    # Describe dataset like mean, min, max, etc.\n    # print(dataframe.describe())\n\n\ndef read_data(file_path):\n\n    \"\"\"\n    This function is the ETL process form the STI values\n    Args:\n        file_path: URL pointing to the pickle file\n    Returns:\n        A pandas dataframe\n    \"\"\"\n\n    df.rename(columns = {'c1': 'Room-id', \n                         'c2': 'STI', \n                         'c3': 'Freq - [Hz]', \n                         'c4': 'PSD(RIR)', \n                         'c5': 'PSD(BGN)' }, \n                         inplace=True)\n        \n    # This is very important otherwise the model will not fit and loss\n    # will show up as NAN\n    label_encoder = LabelEncoder()\n    n_bins = 10 # number of classes\n\n    # number of classes \n    y = label_encoder.fit_transform(pd.cut(df['STI'],\n                                           n_bins, \n                                           retbins=True)[0]\n                                   )\n\n    df['STI'] = y   \n    df.dropna(axis=0, how='any', inplace=True)\n\n    return df\n\n\ndef convert_to_float(x):\n\n    try:\n        return np.float(x)\n    except:\n        return np.nan\n\n\ndef feature_normalize(dataset):\n\n    mu = np.mean(dataset, axis=0)\n    sigma = np.std(dataset, axis=0)\n    return (dataset - mu)/sigma\n\n\ndef plot_axis(ax, x, y, title):\n\n    ax.plot(x, y)\n    ax.set_title(title)\n    ax.xaxis.set_visible(False)\n    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n    ax.set_xlim([min(x), max(x)])\n    ax.grid(True)\n\n\ndef plot_activity(activity, data):\n\n    fig, (ax0, ax1) = plt.subplots(nrows=2,\n         figsize=(15, 10),\n         sharex=True)\n    plot_axis(ax0, data['Freq - [Hz]'], data['PSD(RIR)'], 'PSD(RIR)')\n    plot_axis(ax1, data['Freq - [Hz]'], data['PSD(BGN)'], 'PSD(BGN)')\n    plt.subplots_adjust(hspace=0.2)\n    fig.suptitle(activity)\n    plt.subplots_adjust(top=0.90)\n    # plt.show()\n\n\ndef create_segments_and_labels(df, time_steps, step, label_name):\n\n    \"\"\"\n    This function receives a dataframe and returns the reshaped segments\n    of BGN, RIR data as well as the corresponding STI labels\n    Args:\n        df: Dataframe in the expected format\n        time_steps: Integer value of the length of a segment that is created\n    Returns:\n        reshaped_segments\n        labels:\n    \"\"\"\n\n    N_FEATURES = 2\n    # Number of steps to advance in each iteration (it should always\n    # be equal to the time_steps in order to have no overlap between segments)\n    # step = time_steps\n    \n    segments = []\n    labels = []\n    \n    for i in range(0, len(df) - time_steps, step):\n        xs = df['PSD(RIR)'].values[i: i + time_steps]\n        ys = df['PSD(BGN)'].values[i: i + time_steps]\n        xs = xs[1:]\n        ys = ys[1:]\n        # Retrieve the most often used label in this segment\n        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n        segments.append([xs, ys])\n        labels.append(label)\n\n    # Bring the segments into a better shape\n    reshaped_segments = np.asarray(segments, \n                                   dtype= np.float32).reshape(-1, \n                                                              time_steps-1, \n                                                              N_FEATURES)\n    labels = np.asarray(labels)\n\n    return reshaped_segments, labels\n\n\"\"\"\n\n## Create training database\n\"\"\"\n\n# Set some standard parameters upfront\n\npd.options.display.float_format = '{:.1f}'.format\n\n# sns.set() # Default seaborn look and feel\n# plt.style.use('ggplot')\n\nprint('keras version ', keras.__version__)\n\nLABELS = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n\n# The number of steps within one frequenct range segment\nTIME_PERIODS = len(np.arange(0,8001,10))\nSTEP_DISTANCE = TIME_PERIODS\n\nprint(\"\\n--- Load, inspect and transform data ---\\n\")\n\nprint(\"Load data set from the npz\")\nread_data(df)\n\n# Describe the data\nshow_basic_dataframe_info(df, 20)\n\n(df['STI']).value_counts().plot(kind='bar', \n                                title='Training Examples by STI classes')\n\nplt.savefig('STI_distribution_source_domain.pdf')\n# plt.show()\n\nprint(\"\\n--- Reshape the data into segments ---\\n\")\n\n# Differentiate between test set and training set\ndf_train = df[df['Room-id'] <= 25000]\ndf_test = df[df['Room-id'] > 25000]\n\n# Normalize features for training data set\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ncolumn_names_to_normalize = ['PSD(RIR)','PSD(BGN)']\n\nx = df_train[column_names_to_normalize].values\nx_scaled = scaler.fit_transform(x)\n\ndf_temp = pd.DataFrame(x_scaled, \n                       columns=column_names_to_normalize, \n                       index=df_train.index)\n\ndf_train[column_names_to_normalize] = df_temp\n\n# Now for the test \nx = df_test[column_names_to_normalize].values\nx_scaled = scaler.fit_transform(x)\ndf_temp = pd.DataFrame(x_scaled,\n                       columns=column_names_to_normalize, \n                       index=df_test.index)\n\ndf_test[column_names_to_normalize] = df_temp\n\n# Round in order to comply to NSNumber from RIR curvers\ndf_train = df_train.round({'PSD(RIR)': 6, 'PSD(BGN)': 6})\ndf_test =  df_test.round({'PSD(RIR)': 6, 'PSD(BGN)': 6})\n\n# Reshape the training data into segments, so that \n# they can be processed by the network\n# Define column name of the label vector\n\nLABEL = \"STI\"\n\nx_train, y_train = create_segments_and_labels(df_train,\n                                              TIME_PERIODS,\n                                              STEP_DISTANCE,\n                                              LABEL)\n\n\nx_test, y_test = create_segments_and_labels(df_test,\n                                            TIME_PERIODS,\n                                            STEP_DISTANCE,\n                                            LABEL)","metadata":{"_uuid":"f5a7a06e-3320-4d64-81a0-d8829b17c62c","_cell_guid":"0f3c13be-633d-4857-884e-414800044c26","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:10:51.397438Z","iopub.execute_input":"2023-05-07T19:10:51.398056Z","iopub.status.idle":"2023-05-07T19:12:29.123123Z","shell.execute_reply.started":"2023-05-07T19:10:51.397994Z","shell.execute_reply":"2023-05-07T19:12:29.122221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n\n# Inspect x data\nprint('x_train shape: ', x_train.shape)\nprint(x_train.shape[0], 'training samples')\n\n# Inspect y data\nprint('y_train shape: ', y_train.shape)\n\nprint('y_test shape: ', y_test.shape)\n\n# Set input_shape / reshape for Keras\n\nprint(\" Verifying the training dataset dimensions\")\n\nprint(\"Dimensoes antes\")\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\n# Input image dimensions\nimg_rows = 1600\n\n# Channels go last for TensorFlow backend\nprint(\"Dimensão após restruturacao - treino\")\nx_train_reshaped = x_train.reshape(x_train.shape[0], img_rows)\nprint(x_train_reshaped.shape)\n\n# Now verifies the test\nprint(\"Dimensão após restruturacao - teste\")\nx_test_reshaped = x_test.reshape(x_test.shape[0], img_rows)\nprint(x_test_reshaped.shape)\n\nx_tr = x_train_reshaped\nx_te = x_test_reshaped","metadata":{"_uuid":"2283e7d6-3dd9-4b15-aca6-daab54f51e72","_cell_guid":"f23a3c4c-05f7-4f29-a53c-058fdd53f746","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:12:29.124440Z","iopub.execute_input":"2023-05-07T19:12:29.124770Z","iopub.status.idle":"2023-05-07T19:12:29.134305Z","shell.execute_reply.started":"2023-05-07T19:12:29.124736Z","shell.execute_reply":"2023-05-07T19:12:29.132883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 - Estatísticas de treinamento para o domínio de origem\n#### 5.1 - Training Statistics for the Source Domain","metadata":{"_uuid":"75f8c58f-9710-44dd-a581-0ef03f3b5bb7","_cell_guid":"c2b370e6-3e24-4fd4-a8b7-f27ea1e28660","trusted":true}},{"cell_type":"code","source":"### Fitting \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Extract the column from the dataframe \n# and convert it to a numpy array\ndata = (df['STI']/10).values\n\n# Fit a Gaussian distribution to the data\nmu, std = stats.norm.fit(data)\n\n# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='b')\n\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"9ccb38f0-5463-40f1-aa69-cdd0310a8b5e","_cell_guid":"d5a03854-8b35-4e92-afb7-fd90a0d8e230","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:12:29.139116Z","iopub.execute_input":"2023-05-07T19:12:29.139642Z","iopub.status.idle":"2023-05-07T19:12:29.835911Z","shell.execute_reply.started":"2023-05-07T19:12:29.139605Z","shell.execute_reply":"2023-05-07T19:12:29.835150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='b')\n\n# Plot the Gaussian fit\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, mu, std)\n\n#plt.plot(x, c, 'r', linewidth=2)\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\nax2 = plt.twinx()\n\n# plot the data for the second y-axis\nax2.plot(x, p, 'r--')\nax2.set_ylabel('Função densidade acumulada', color='r')\n\n# show the plot\nplt.show()\n\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"21eaf63a-c06d-4779-9c43-2f1661c034fd","_cell_guid":"4d1e9235-9040-4704-b296-1729802137f0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:12:29.837455Z","iopub.execute_input":"2023-05-07T19:12:29.837811Z","iopub.status.idle":"2023-05-07T19:12:30.404744Z","shell.execute_reply.started":"2023-05-07T19:12:29.837774Z","shell.execute_reply":"2023-05-07T19:12:30.403855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='b')\n\n# Plot the Gaussian fit\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\nc = stats.norm.cdf(x, mu, std)\n\n#plt.plot(x, c, 'r', linewidth=2)\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\nax2 = plt.twinx()\n\n# plot the data for the second y-axis\nax2.plot(x, c, 'r--')\nax2.set_ylabel('Função densidade acumuldada', color='r')\n\n# show the plot\nplt.show()\n\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"bbfe00ca-3324-4f6f-bc8f-1cacaa468cda","_cell_guid":"ca30d99b-6e32-4cc2-a8fd-a5dc241923de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:12:30.406212Z","iopub.execute_input":"2023-05-07T19:12:30.406793Z","iopub.status.idle":"2023-05-07T19:12:30.971313Z","shell.execute_reply.started":"2023-05-07T19:12:30.406755Z","shell.execute_reply":"2023-05-07T19:12:30.969783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 -  Treinamento da rede autoencoder variacional\n#### 5.2 - VAE training","metadata":{"_uuid":"5130ae11-b28d-41ce-873d-86a1e7f4baf1","_cell_guid":"31dc3c73-9bba-4d76-ba65-45fe82a36782","trusted":true}},{"cell_type":"code","source":"\"\"\"## 3 - Reescaling the data for the VAE traning\"\"\"\n\nprint(\" Build the VAE models and set is parameters \")\nbatch_size, n_epoch = 1, 5\nn_hidden, z_dim = 64, 2 # Here changes the z-dimension (latent space)\n\n# encoder\nx = Input(shape=(x_tr.shape[1:]))\nx_encoded = Dense(n_hidden, activation='relu')(x)\nx_encoded = Dense(n_hidden//2, activation='relu')(x_encoded)\n\nmu = Dense(z_dim)(x_encoded)\nlog_var = Dense(z_dim)(x_encoded)\n\n# sampling function\ndef sampling(args):\n    mu, log_var = args\n    eps = K.random_normal(shape=(batch_size, z_dim), mean=0., stddev=1.0)\n    return mu + K.exp(log_var) * eps\n\nz = Lambda(sampling, output_shape=(z_dim,))([mu, log_var])\n\n# decoder\nz_decoder1 = Dense(n_hidden//2, activation='relu')\nz_decoder2 = Dense(n_hidden, activation='relu')\ny_decoder = Dense(x_tr.shape[1], activation='sigmoid')\n\nz_decoded = z_decoder1(z)\nz_decoded = z_decoder2(z_decoded)\ny = y_decoder(z_decoded)\n\n# loss\nreconstruction_loss = objectives.binary_crossentropy(x, y) * x_tr.shape[1]\nkl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\nvae_loss = reconstruction_loss + kl_loss\n\n# build model\nvae = Model(x, y)\nvae.add_loss(vae_loss)\n\n# Add metrics\nvae.add_metric(reconstruction_loss, name='reconstruction_loss')\nvae.add_metric(kl_loss, name='kl_loss')\nvae.add_metric(vae_loss, name='vae_loss')\n\nvae.compile(optimizer='rmsprop')\nvae.summary()\n\n# train\nvae.fit(x_tr,\n       shuffle=True,\n       epochs=n_epoch,\n       batch_size=batch_size,\n       validation_data=(x_te, None), verbose=1)\n\n# build encoder\nencoder = Model(x, mu)\nencoder.summary()","metadata":{"_uuid":"325be844-7a3e-4156-bc7f-6edbd2c8c48d","_cell_guid":"8bda688a-2bdf-40ab-858a-f884909a3ab1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:12:30.972665Z","iopub.execute_input":"2023-05-07T19:12:30.972995Z","iopub.status.idle":"2023-05-07T19:18:30.165194Z","shell.execute_reply.started":"2023-05-07T19:12:30.972959Z","shell.execute_reply":"2023-05-07T19:18:30.164494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 - Valores da função perda da rede VAE\n#### 5.3 - VAE losses values","metadata":{"_uuid":"afeb11b5-d68e-49a1-91d9-903276a2cce3","_cell_guid":"1d5b1d3f-4cc2-4ca6-b509-c44fb5c4ab22","trusted":true}},{"cell_type":"code","source":"vae.history","metadata":{"_uuid":"b29c56e9-19ff-43b8-b85e-606e04ed8e35","_cell_guid":"40a2e51d-8840-4127-8f88-493911b96cfc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.168270Z","iopub.execute_input":"2023-05-07T19:18:30.168535Z","iopub.status.idle":"2023-05-07T19:18:30.177278Z","shell.execute_reply.started":"2023-05-07T19:18:30.168509Z","shell.execute_reply":"2023-05-07T19:18:30.176290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list all data in history\nprint(vae.history.history.keys())","metadata":{"_uuid":"4c964d28-8ee4-4ed3-85bc-062a402a6834","_cell_guid":"3b221bed-aed5-459f-b643-7d0183fd0bdb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.178535Z","iopub.execute_input":"2023-05-07T19:18:30.178843Z","iopub.status.idle":"2023-05-07T19:18:30.185485Z","shell.execute_reply.started":"2023-05-07T19:18:30.178810Z","shell.execute_reply":"2023-05-07T19:18:30.184481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vae.history.history['loss'])","metadata":{"_uuid":"58961d67-a093-49ff-b6de-55a7b7b9dcc4","_cell_guid":"22c1cbcd-77b2-414d-89b0-4a2b0cd98aab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.186629Z","iopub.execute_input":"2023-05-07T19:18:30.186987Z","iopub.status.idle":"2023-05-07T19:18:30.198907Z","shell.execute_reply.started":"2023-05-07T19:18:30.186951Z","shell.execute_reply":"2023-05-07T19:18:30.198222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the loss values from the history and plot it\nloss_values = vae.history.history['loss']\n\n# Plot the loss values over time\nplt.plot(loss_values)\nplt.xlabel('Época')\nplt.ylabel('Função de perda')\nplt.show()","metadata":{"_uuid":"c9935fbe-c6bb-4910-b620-b5a03d70c5de","_cell_guid":"4058ed13-0f26-4594-bd1b-ad883bb604f8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.202057Z","iopub.execute_input":"2023-05-07T19:18:30.202335Z","iopub.status.idle":"2023-05-07T19:18:30.347905Z","shell.execute_reply.started":"2023-05-07T19:18:30.202306Z","shell.execute_reply":"2023-05-07T19:18:30.347170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#List of all possible metrics to plot: \n\n'''\ndict_keys(['loss', 'reconstruction_loss', \n           'kl_loss', 'vae_loss', \n           'val_loss', 'val_reconstruction_loss', \n           'val_kl_loss', 'val_vae_loss'])\n'''\n\n# Extract the loss values from the history\ncomponent1_values = vae.history.history['reconstruction_loss']\ncomponent2_values = vae.history.history['val_reconstruction_loss']\n\n# Plot the loss values over time\nplt.plot(component1_values, 'go-', label='Função perda de reconstrução')\nplt.plot(component2_values, label='Validação - Função perda de reconstrução')\nplt.xlabel('Época')\nplt.ylabel('Função de perda')\nplt.legend()\nplt.show()","metadata":{"_uuid":"832903a9-88a5-42cc-99ca-ce025eac854f","_cell_guid":"b31035e8-c792-4c8b-82a4-1a57da1dd752","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.350926Z","iopub.execute_input":"2023-05-07T19:18:30.351199Z","iopub.status.idle":"2023-05-07T19:18:30.519268Z","shell.execute_reply.started":"2023-05-07T19:18:30.351170Z","shell.execute_reply":"2023-05-07T19:18:30.518328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the KL loss values from the history and plot it\ncomponent1_values = vae.history.history['kl_loss']\ncomponent2_values = vae.history.history['val_kl_loss']\n\n# Plot the loss values over time\nplt.plot(component1_values, 'g--', label='Função perda de KL')\nplt.plot(component2_values, label='Validação - Função perda de KL')\n\nplt.xlabel('Época')\nplt.ylabel('Função de perda')\nplt.legend()\nplt.show()","metadata":{"_uuid":"7e4f66de-7024-4b0a-94d0-b628a2c2dcb2","_cell_guid":"fc5cca06-4251-443d-a0b1-4c8533f58ee2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.522335Z","iopub.execute_input":"2023-05-07T19:18:30.522599Z","iopub.status.idle":"2023-05-07T19:18:30.699928Z","shell.execute_reply.started":"2023-05-07T19:18:30.522572Z","shell.execute_reply":"2023-05-07T19:18:30.699023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the loss values from the history\ncomponent1_values = vae.history.history['vae_loss']\ncomponent2_values = vae.history.history['val_vae_loss']\n\n# Plot the loss values over time\nplt.plot(component1_values, 'g--', label='VAE - Função perda total')\nplt.plot(component2_values, label='Validação VAE - Função perda total')\n\nplt.xlabel('Época')\nplt.ylabel('Função de perda')\nplt.legend()\nplt.show()","metadata":{"_uuid":"2104b97f-698f-479a-96ef-28394915afe9","_cell_guid":"638158dc-2e43-4062-912b-58a79e19c5f4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.701285Z","iopub.execute_input":"2023-05-07T19:18:30.701787Z","iopub.status.idle":"2023-05-07T19:18:30.859697Z","shell.execute_reply.started":"2023-05-07T19:18:30.701747Z","shell.execute_reply":"2023-05-07T19:18:30.858795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6 - Otimização de hiperparâmetros (HPO) do modelo VAE\n### 6 - Hyperparameter Optimization (HPO) of the VAE model","metadata":{"_uuid":"75286eaf-b92a-49f8-a36a-17b2005fd115","_cell_guid":"bb24955a-8795-42e7-be43-fdab96f459b9","trusted":true}},{"cell_type":"code","source":"# You can use Matplotlib instead of Plotly for visualization by \n#  simply replacing `optuna.visualization` with\n# `optuna.visualization.matplotlib` in the following examples.\n\nimport optuna\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\n\nSEED = 42\n\nnp.random.seed(SEED)\n\nfrom keras.backend import clear_session\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"a7618d88-2de1-4b41-a09a-75ec440a0d7f","_cell_guid":"8a593479-049e-4073-850b-35525f424b3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:30.861001Z","iopub.execute_input":"2023-05-07T19:18:30.861523Z","iopub.status.idle":"2023-05-07T19:18:31.288537Z","shell.execute_reply.started":"2023-05-07T19:18:30.861485Z","shell.execute_reply":"2023-05-07T19:18:31.287726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def start_Autoencoder(features,trials,  plot_graph = False):\n\n  # Variational Autoencoder structure\n  def create_model(activation, layers_num, zdim_num, kernel_initializer):\n    clear_session()\n    \n    batch_size, n_epoch = 1, 1\n    n_hidden, z_dim = layers_num, zdim_num \n    # Here change the dimension (Model hyperparameter)\n\n    # encoder\n    x = Input(shape=(x_tr.shape[1:]))\n    x_encoded = Dense(n_hidden, \n                      activation=activation, \n                      kernel_initializer=kernel_initializer)(x)\n    x_encoded = Dense(n_hidden//2, activation=activation)(x_encoded)\n\n    mu = Dense(z_dim)(x_encoded)\n    log_var = Dense(z_dim)(x_encoded)\n\n    # sampling function\n    def sampling(args):\n        mu, log_var = args\n        eps = K.random_normal(shape=(batch_size, z_dim), mean=0., stddev=1.0)\n        return mu + K.exp(log_var) * eps\n\n    z = Lambda(sampling, output_shape=(z_dim,))([mu, log_var])\n\n    # decoder\n    z_decoder1 = Dense(n_hidden//2, activation='relu')\n    z_decoder2 = Dense(n_hidden, activation='relu')\n    y_decoder = Dense(x_tr.shape[1], activation='sigmoid')\n\n    z_decoded = z_decoder1(z)\n    z_decoded = z_decoder2(z_decoded)\n    y = y_decoder(z_decoded)\n\n    # loss\n    reconstruction_loss = objectives.binary_crossentropy(x, y) * x_tr.shape[1]\n    kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)\n    vae_loss = reconstruction_loss + kl_loss\n\n    # build model\n    vae = Model(x, y)\n    \n    vae.add_loss(vae_loss)\n    # Add metrics\n    vae.add_metric(reconstruction_loss, name='reconstruction_loss')\n    vae.add_metric(kl_loss, name='kl_loss')\n    vae.add_metric(vae_loss, name='vae_loss')\n    autoencoder = vae\n\n    return autoencoder\n\n  # Objective function to optimize by OPTUNA module\n    \n  def objective(trial):\n    activation = trial.suggest_categorical(\"activation\", \n                                           [\"relu\", \"sigmoid\", \"swish\"])\n    layers_num = trial.suggest_int(\"layers_num\", 2,64)\n    zdim_num = trial.suggest_int(\"dim_latent_z\", 2,128)\n    if (activation == \"relu\"):\n      model = create_model(activation, \n                           layers_num, \n                           zdim_num, \n                           kernel_initializer=\"HeUniform\")\n    else:\n      model = create_model(activation,\n                           layers_num,\n                           zdim_num,\n                           kernel_initializer=\"GlorotUniform\")\n    \n    model.compile(optimizer='rmsprop')\n    \n    # Implement early stopping criterion. \n    # Training process stops when there is no improvement during 50 iterations\n    \n    callback = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n    history = model.fit(features, \n                        features,\n                        batch_size = 1,\n                        epochs=1,\n                        callbacks = [callback], \n                        verbose = 0)\n    \n    return history.history[\"loss\"][-1]\n  \n  study = optuna.create_study(direction='minimize')\n  study.optimize(objective, n_trials=trials)\n    \n  # Create final model with the best hyperparams\n  print('Best hyperparams found by Optuna: \\n', study.best_params)\n  if (study.best_params['activation'] == \"relu\"):\n    model = create_model(study.best_params['activation'],\n                         int(study.best_params['layers_num']),\n                         int(study.best_params['dim_latent_z']),\n                         kernel_initializer=\"HeUniform\")\n  else:\n    model = create_model(study.best_params['activation'],\n                        int(study.best_params['layers_num']),\n                        int(study.best_params['dim_latent_z']),\n                        kernel_initializer=\"GlorotUniform\")\n\n  model.compile(optimizer='rmsprop')\n  model.summary()\n    \n  # Implement early stopping criterion. \n  # Training process stops when there is no improvement during a certatin number\n  # of iterations\n    \n  callback = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n  history = model.fit(features, \n                      features,\n                      batch_size = 1,\n                      epochs=2, \n                      callbacks = [callback],\n                      verbose = 0)\n\n  result = model.predict(features)\n    \n  # Result evaluation\n  print(f'RMSE Autoencoder: {np.sqrt(mean_squared_error(features, result))}')\n  print('')\n\n  # Following values are returned: extracted_f || MSE || OPTUNA best hyperparams\n\n  return mean_squared_error(features, result), study.best_params, study\n\n\n# Run the HPO on the VAE\n\nAcoder_MSE, Acoder_hyperparams, Study = start_Autoencoder(features = x_tr,\n                                  trials = 5,\n                                  plot_graph=True)","metadata":{"_uuid":"53deb51c-e33e-4924-b6d6-9478f59fbbd6","_cell_guid":"cef69188-f5d3-4b29-96c9-662277532d8b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:18:31.291268Z","iopub.execute_input":"2023-05-07T19:18:31.291811Z","iopub.status.idle":"2023-05-07T19:24:09.969603Z","shell.execute_reply.started":"2023-05-07T19:18:31.291770Z","shell.execute_reply":"2023-05-07T19:24:09.968650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_optimization_history(Study)","metadata":{"_uuid":"0ad2f9ef-5b28-47aa-bda9-89db25ce552e","_cell_guid":"4567dfe9-3e0b-4f14-ab20-b5dc50ab68a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:09.971098Z","iopub.execute_input":"2023-05-07T19:24:09.971458Z","iopub.status.idle":"2023-05-07T19:24:10.119622Z","shell.execute_reply.started":"2023-05-07T19:24:09.971422Z","shell.execute_reply":"2023-05-07T19:24:10.118816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_param_importances(Study)","metadata":{"_uuid":"6a593f15-f2f4-4dc1-bfe7-30a55d088328","_cell_guid":"6454f624-3f08-4576-959c-670a343b4799","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.120819Z","iopub.execute_input":"2023-05-07T19:24:10.121141Z","iopub.status.idle":"2023-05-07T19:24:10.294547Z","shell.execute_reply.started":"2023-05-07T19:24:10.121114Z","shell.execute_reply":"2023-05-07T19:24:10.293830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_param_importances(\n    Study, target=lambda t: t.duration.total_seconds(), target_name=\"duration\"\n)","metadata":{"_uuid":"f5ce83bd-e5e4-4d39-99cf-73cae5892e03","_cell_guid":"04ef1436-fca1-497d-8da3-82807c8ca328","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.295865Z","iopub.execute_input":"2023-05-07T19:24:10.296189Z","iopub.status.idle":"2023-05-07T19:24:10.452922Z","shell.execute_reply.started":"2023-05-07T19:24:10.296156Z","shell.execute_reply":"2023-05-07T19:24:10.452065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_edf(Study)","metadata":{"_uuid":"10c3eb50-adf1-464b-80e7-546719923148","_cell_guid":"5b59313e-c951-4979-a0ed-f567241cad3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.455210Z","iopub.execute_input":"2023-05-07T19:24:10.455799Z","iopub.status.idle":"2023-05-07T19:24:10.471226Z","shell.execute_reply.started":"2023-05-07T19:24:10.455761Z","shell.execute_reply":"2023-05-07T19:24:10.470168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_contour(Study)","metadata":{"_uuid":"763eebdd-b626-4395-be3b-89d42f5614c5","_cell_guid":"8d4685c0-b4ff-47ad-bd23-60572f4f3852","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.472745Z","iopub.execute_input":"2023-05-07T19:24:10.473090Z","iopub.status.idle":"2023-05-07T19:24:10.779995Z","shell.execute_reply.started":"2023-05-07T19:24:10.473054Z","shell.execute_reply":"2023-05-07T19:24:10.779071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7 - Visualização dos Embeddings VAE\n### 7 - Visualization of the VAE Embeddings","metadata":{"_uuid":"068413f3-aa80-4339-ab19-b79276c206c4","_cell_guid":"8f1f7507-747f-472f-9fe4-e7e33619e044","trusted":true}},{"cell_type":"code","source":"import matplotlib\n\ncmap = matplotlib.cm.get_cmap('Spectral')","metadata":{"_uuid":"7ecd84ca-5167-4fb4-a7c9-eaef60f84528","_cell_guid":"5f0a324b-0584-4fe0-8121-049c31508603","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.781342Z","iopub.execute_input":"2023-05-07T19:24:10.781693Z","iopub.status.idle":"2023-05-07T19:24:10.785962Z","shell.execute_reply.started":"2023-05-07T19:24:10.781657Z","shell.execute_reply":"2023-05-07T19:24:10.785089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap","metadata":{"_uuid":"7c1bcfc3-8c36-4717-8ac0-bc59d422b2dc","_cell_guid":"9d74e5c5-aecc-4790-86d0-4b594d644cb9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.787588Z","iopub.execute_input":"2023-05-07T19:24:10.787976Z","iopub.status.idle":"2023-05-07T19:24:10.799779Z","shell.execute_reply.started":"2023-05-07T19:24:10.787941Z","shell.execute_reply":"2023-05-07T19:24:10.798821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_representation(features, labels, rep_type):\n    fig, axes = plt.subplots(2, 3, figsize=(12,6))\n    fig.suptitle('Scatter plot: ' + rep_type + ' 4 features representation', fontsize=16)\n\n    for i, pair in enumerate(combinations([0,1,2,3], 2)):\n        for label, color in zip([1,2,3,4,5,6,7,8,9], ['blue', 'green',\n                                                      'black', 'yellow', \n                                                      'brown', 'orange',\n                                                      'red', 'gray', \n                                                      'olive','gold']):\n            \n            axes[int(i / 3), int(i % 3)].scatter(features[labels == label, pair[0]], \n                                                features[labels == label, pair[1]], \n                                                c=color,\n                                                alpha=0.5,\n                                                label = label)\n            \n            axes[int(i / 3), int(i % 3)].set_xlabel(f'Feature {pair[0]}')\n            axes[int(i / 3), int(i % 3)].set_ylabel(f'Feature {pair[1]}')\n            axes[int(i / 3), int(i % 3)].legend()\n            \n    plt.tight_layout()","metadata":{"_uuid":"6a94827e-4874-4d0b-8b91-86aead1c739b","_cell_guid":"4da70ea1-5b6b-433a-ab57-17d9f46501f3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.801266Z","iopub.execute_input":"2023-05-07T19:24:10.801599Z","iopub.status.idle":"2023-05-07T19:24:10.811914Z","shell.execute_reply.started":"2023-05-07T19:24:10.801562Z","shell.execute_reply":"2023-05-07T19:24:10.811293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE, Isomap\nfrom sklearn.decomposition import PCA\nfrom itertools import combinations","metadata":{"_uuid":"b76416a9-52c8-4a9d-918c-1da489342f51","_cell_guid":"ac3cde49-63a2-45c7-b5d9-dd411e859350","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.813332Z","iopub.execute_input":"2023-05-07T19:24:10.813800Z","iopub.status.idle":"2023-05-07T19:24:10.825522Z","shell.execute_reply.started":"2023-05-07T19:24:10.813760Z","shell.execute_reply":"2023-05-07T19:24:10.824504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PCA_transformer = PCA(n_components=4)\nx_train_reshaped_TSE = np.asarray(x_train_reshaped, dtype='float64')\nPCA_representation = PCA_transformer.fit_transform(x_train_reshaped_TSE)\n\n# Visualize the results of the dimensionaly reduction\nplot_representation(PCA_representation, y_train, 'PCA')","metadata":{"_uuid":"fe86954b-4a03-4b3c-bc05-32352fedda78","_cell_guid":"57e83798-281f-4742-b992-338c31ec38c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:10.834265Z","iopub.execute_input":"2023-05-07T19:24:10.834518Z","iopub.status.idle":"2023-05-07T19:24:20.298496Z","shell.execute_reply.started":"2023-05-07T19:24:10.834494Z","shell.execute_reply":"2023-05-07T19:24:20.297562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8 - Salvando o modelo do codificador em uma extensão de arquivo .h5\n### 8 - Saving the Encoder Model to a .h5 File Extension","metadata":{"_uuid":"aadb28d2-209b-4f32-9d7f-c552762da3a0","_cell_guid":"ac251834-a2a9-41fe-9902-cbd9a6af39b4","trusted":true}},{"cell_type":"code","source":"# Save weights approaches\n\n#vae.save_weights('my_vae_weights.h5')\n#encoder.save(\"my_h5_model.h5\")\n\n# Load weights \n# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n#encoder.save(\"my_h5_model.h5\")\n\n# It can be used to reconstruct the model identically.\n#reconstructed_model = keras.models.load_model(\"my_h5_model.h5\")\n\n# Let's check: (Test to verify if the model was reload properly)\n#np.testing.assert_allclose(\n#    encoder.predict(test_input), reconstructed_model.predict(test_input)\n#)\n\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\n#reconstructed_model.fit(test_input, test_target)","metadata":{"_uuid":"3c7471d3-7e13-42d5-82c3-06dce5ff5b3a","_cell_guid":"d57da243-b456-4706-b417-6947b71e582f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:20.300297Z","iopub.execute_input":"2023-05-07T19:24:20.300806Z","iopub.status.idle":"2023-05-07T19:24:20.305147Z","shell.execute_reply.started":"2023-05-07T19:24:20.300769Z","shell.execute_reply":"2023-05-07T19:24:20.304130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\nencoder.save(\"vae_encoder.h5\")\n\n# It can be used to reconstruct the model identically.\n\n# Load the saved model and make predicitions\nreconstructed_model = keras.models.load_model(\"vae_encoder.h5\")\nx_te_latent_recons = reconstructed_model.predict(x_te, batch_size=1)\n\n# Compare with the orginal encoder model\nx_te_latent_origin = encoder.predict(x_te, batch_size=batch_size)\n\n# Compare the results\nx_te_latent_origin == x_te_latent_recons","metadata":{"_uuid":"721470f6-2f10-4550-87c1-6d8c950e40a0","_cell_guid":"b9713a5e-6699-4f55-9844-fac5b5e371db","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:20.306617Z","iopub.execute_input":"2023-05-07T19:24:20.307209Z","iopub.status.idle":"2023-05-07T19:24:26.318279Z","shell.execute_reply.started":"2023-05-07T19:24:20.307128Z","shell.execute_reply":"2023-05-07T19:24:26.317348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 - Salvando o modelo do codificador como um objeto gráfico do TensorFlow\n#### 8.2 Saving the Encoder Model as a TensorFlow Graph Object","metadata":{"_uuid":"6be19cf1-6f38-45b0-9447-1dc4d6948236","_cell_guid":"eb8fedd3-0f54-4705-a4dc-ef24ddef1919","trusted":true}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Save the model\n# encoder.save(\"my_model.model\")\nencoder.save(\"my_model.model\")\n\n# Load the saved model\ntensorflow_graph = tf.saved_model.load(\"./my_model.model\")\n\n# Make predictions \nz_latent = tensorflow_graph(x_te ,False, None).numpy()\n\n# x = np.random.uniform(size=(1600))\n# x = np.expand_dims(x, axis=0)\n# predicted = tensorflow_graph(x ,False, None).numpy()\n\n# predicted\n# Compare the results of the saved model and the original model in memory\nnp.round(z_latent,4) == np.round(x_te_latent_origin,4)","metadata":{"_uuid":"94160d19-836d-4510-b239-56021ba43117","_cell_guid":"04982108-3bc1-4fb5-8335-247ad916299a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:26.319656Z","iopub.execute_input":"2023-05-07T19:24:26.320008Z","iopub.status.idle":"2023-05-07T19:24:27.087749Z","shell.execute_reply.started":"2023-05-07T19:24:26.319972Z","shell.execute_reply":"2023-05-07T19:24:27.086826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.3 - Salvando as previsões do conjunto de dados de treinamento (x_tr_latent) no espaço codificado para modelagem KCPR\n#### 8.3 - Saving the Training Dataset Predictions (x_tr_latent) to the Encoded Space for KCPR Modeling","metadata":{"_uuid":"d39bc48f-4040-4c81-b3c1-301010d77651","_cell_guid":"8443a4e4-24ba-4af9-8df8-e2382aff38da","trusted":true}},{"cell_type":"code","source":"# Important: Generate the z latent encoding  with 128 dimensions, then z = 12\n\n# x_tr\nx_tr_latent = encoder.predict(x_tr, batch_size=batch_size)\n\n# Make a pandas dataframe of the latent space and export the data \nx_tr_latent.shape\n\n# Create the padnas dataframe \ndataset_high_z_dim = pd.DataFrame(x_tr_latent)\nclasses = pd.DataFrame(y_train)\n\n# dataset_high_z_dim.append(classes)\ndataset_high_z_dim['classes'] = classes\nprint(dataset_high_z_dim)\n\n# print(classes)\ndataset_high_z_dim.to_csv('dataset_high_z_dim.csv')","metadata":{"_uuid":"b073c67e-2120-47f9-b604-1b4c556bffd0","_cell_guid":"9807467e-3a24-4263-927f-b684dc79042c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:27.091160Z","iopub.execute_input":"2023-05-07T19:24:27.091754Z","iopub.status.idle":"2023-05-07T19:24:42.694546Z","shell.execute_reply.started":"2023-05-07T19:24:27.091711Z","shell.execute_reply":"2023-05-07T19:24:42.693757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9 - Plotando a visualização do espaço do kernel 2D\n### 9 -  Plotting 2D kernel space visualization","metadata":{"_uuid":"2680c082-4a3c-447b-8d9e-d1f6a6c2ac44","_cell_guid":"00b10120-40fb-4bf8-9053-fded470cfac6","trusted":true}},{"cell_type":"code","source":"#Inspecting the latent space varialbes (z1 and z2)\nx_tr_latent","metadata":{"_uuid":"4b05b891-571c-4080-869e-56344aa4aa0b","_cell_guid":"46c746d7-0f82-4817-8b9d-b404a5c5a84b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:42.695858Z","iopub.execute_input":"2023-05-07T19:24:42.696183Z","iopub.status.idle":"2023-05-07T19:24:42.705310Z","shell.execute_reply.started":"2023-05-07T19:24:42.696149Z","shell.execute_reply":"2023-05-07T19:24:42.701711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.1 - Plotando a representação do espaço latente (z = 2) do treinamento (conjunto de dados de origem)\n#### 9.1 Plotting the Latent Space Representation (z = 2) of the Training (Source Dataset)","metadata":{"_uuid":"95a1bc77-7f00-4988-a305-eb8816e3932f","_cell_guid":"853362b1-7866-4e47-afc2-f15a7e6a224a","trusted":true}},{"cell_type":"code","source":"# Plot\nplt.figure(figsize=(6, 6))\ny_tr = y_train\nplt.scatter(x_tr_latent[:, 0], x_tr_latent[:, 1], c=y_tr/10)\nplt.colorbar()\n\nplt.legend(title=\"STI - Dados de treinamento (domínio) \")\nplt.xlabel('Dimensão - z1')\nplt.ylabel('Dimensão - z2')\n#ax.add_artist(legend1)\n\nplt.show()","metadata":{"_uuid":"c83824c0-494d-49a7-8059-4201407e9d9f","_cell_guid":"4e4d1ef3-f3f4-4b19-9cdc-53b9d43afd9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:42.706484Z","iopub.execute_input":"2023-05-07T19:24:42.706886Z","iopub.status.idle":"2023-05-07T19:24:43.943507Z","shell.execute_reply.started":"2023-05-07T19:24:42.706846Z","shell.execute_reply":"2023-05-07T19:24:43.942467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.2 Plotando a representação do espaço latente (z = 2) do teste (conjunto de dados de origem)\n#### 9.2 Plotting the Latent Space Representation (z = 2) of the Testing (Source Dataset)","metadata":{"_uuid":"d9dde2d8-216b-421b-8d91-021705310131","_cell_guid":"bf044c29-aa1d-47d4-92ef-8f7ec217208f","trusted":true}},{"cell_type":"code","source":"# Make the prediction of the latent space \nx_te_latent = encoder.predict(x_te, batch_size=batch_size)\ny_te = y_test  \nplt.figure(figsize=(6, 6))\nplt.scatter(x_te_latent[:, 0], x_te_latent[:, 1], c = y_te/10 )\nplt.legend(title=\"STI - Dados de validação (domínio) \")\nplt.xlabel('Dimensão - z1')\nplt.ylabel('Dimensão - z2')\nplt.colorbar()\n\n# produce a legend with the unique colors from the scatter\nplt.show()","metadata":{"_uuid":"7e5806a8-078a-45f2-8a0b-50ed1e64864e","_cell_guid":"d2d59e0f-13f0-4162-949b-035adb0dc810","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:43.945076Z","iopub.execute_input":"2023-05-07T19:24:43.945429Z","iopub.status.idle":"2023-05-07T19:24:47.375423Z","shell.execute_reply.started":"2023-05-07T19:24:43.945391Z","shell.execute_reply":"2023-05-07T19:24:47.374582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.3 - Reconstruindo o espaço latente VAE via codificador de mapas embuídos (Domínio de origen)\n#### 9.3 - Reconstructing VAE Latent Space via Encoder Embedding (Source Domain)","metadata":{"_uuid":"47b1dc8d-bbca-4b29-80dd-04f035f36310","_cell_guid":"0dd4ee2a-bd04-4eeb-9b5c-19598f9b051f","trusted":true}},{"cell_type":"code","source":"x_tr_latent = encoder.predict(x_tr, batch_size=batch_size)\ny_tr = y_train                            \ndataset = pd.DataFrame({'z1': x_tr_latent[:, 0], 'z2': x_tr_latent[:, 1], 'STI':  y_tr})\nprint(dataset)","metadata":{"_uuid":"49d57318-ce89-4281-ad3a-1ffa97cd9a4b","_cell_guid":"e7028531-d412-4a55-a2c9-fb98090d56fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:24:47.377316Z","iopub.execute_input":"2023-05-07T19:24:47.377822Z","iopub.status.idle":"2023-05-07T19:25:02.304619Z","shell.execute_reply.started":"2023-05-07T19:24:47.377782Z","shell.execute_reply":"2023-05-07T19:25:02.303148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.1 - Visualização da inferência do histograma do espaço latente\n##### 9.3.1 - Visualization of Latent Space Histogram Inference","metadata":{"_uuid":"f344f532-f4aa-46dd-b225-74d2f5b1658d","_cell_guid":"72effd88-ef71-4cba-9594-e88538a94540","trusted":true}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\ndataset['z1'].plot(kind=\"hist\", bins=20, density=True, alpha=0.6 )\ndataset['STI'].plot(kind=\"hist\", bins=20, density=True, alpha=0.6)\ndataset['z2'].plot(kind=\"hist\", bins=20, density=True, alpha=0.6)\n\nplt.legend(title=\"STI - Dados de validação (domínio)\")\nplt.xlabel('Dimensão latente')\nplt.ylabel('Função densidade de probabilidade')\nplt.show()","metadata":{"_uuid":"fd423151-05d5-498e-b449-6151027de700","_cell_guid":"c35bf54b-cb30-4ecb-8bb6-a705a424372d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:02.307088Z","iopub.execute_input":"2023-05-07T19:25:02.307643Z","iopub.status.idle":"2023-05-07T19:25:02.624166Z","shell.execute_reply.started":"2023-05-07T19:25:02.307593Z","shell.execute_reply":"2023-05-07T19:25:02.623276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.2 - IA Explicativa (EAX) - Plotando a reconstrução do modelo codificado em 2 dimensões\n##### 9.3.2 - Explainable AI (EAX) - Plotting the Encoded Model Reconstruction in 2-Dimensions","metadata":{"_uuid":"89b6692b-c716-4d82-8e59-830b3cb5fd54","_cell_guid":"c74a73d4-c317-4f8a-8ff7-46d151ee9b46","trusted":true}},{"cell_type":"code","source":"# build decoder\ndecoder_input = Input(shape=(z_dim,))\n_z_decoded = z_decoder1(decoder_input)\n_z_decoded = z_decoder2(_z_decoded)\n_y = y_decoder(_z_decoded)\ngenerator = Model(decoder_input, _y)\n\n# generator.summary()\n# display a 2D manifold of the digits\n\nn = 15 # figure with 15x15 dimensional size\ndigit_size1 = 40\ndigit_size2 = 40\nfigure = np.zeros((digit_size1 * n, digit_size2 * n))\n\ngrid_x = norm.ppf(np.linspace(0.05, 0.95, n)) \ngrid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = generator.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size1, digit_size2)\n        figure[i * digit_size1: (i + 1) * digit_size1,\n               j * digit_size2: (j + 1) * digit_size2] = digit\n\nplt.figure(figsize=(10, 10))\nplt.xlabel('Dimensão - z1')\nplt.ylabel('Dimensão - z2')\nplt.imshow(figure, cmap='jet')\nplt.show()","metadata":{"_uuid":"5db486be-6675-4248-9f07-b1b2e7f75c70","_cell_guid":"22146a1c-7de8-4a50-b999-609ce403d67a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:02.625550Z","iopub.execute_input":"2023-05-07T19:25:02.626050Z","iopub.status.idle":"2023-05-07T19:25:10.336711Z","shell.execute_reply.started":"2023-05-07T19:25:02.626012Z","shell.execute_reply":"2023-05-07T19:25:10.335813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.3 PCA aplicado ao espaço latente durante o treinamento de VAE: z Dim = 2\n##### 9.3.3 - PCA Applied to the Latent Space During VAE Training: z Dimension = 2","metadata":{"_uuid":"8d706532-17cc-4e47-bd4a-7c5f939167ed","_cell_guid":"93844983-6946-4ed3-92dd-19f24c608e8d","trusted":true}},{"cell_type":"code","source":"x_te_latent_recons.shape","metadata":{"_uuid":"a3b59f7e-b6b5-4f8d-903c-5a5570cce992","_cell_guid":"8237c233-6d90-4def-bd7c-e2da1e06c990","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:10.338024Z","iopub.execute_input":"2023-05-07T19:25:10.338493Z","iopub.status.idle":"2023-05-07T19:25:10.344339Z","shell.execute_reply.started":"2023-05-07T19:25:10.338458Z","shell.execute_reply":"2023-05-07T19:25:10.343155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_te.min()","metadata":{"_uuid":"88694015-f9bd-4af8-8cb1-c3972dbe806f","_cell_guid":"17bac467-c018-467e-bfc7-a5fb88daa112","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:10.346231Z","iopub.execute_input":"2023-05-07T19:25:10.346592Z","iopub.status.idle":"2023-05-07T19:25:10.359988Z","shell.execute_reply.started":"2023-05-07T19:25:10.346557Z","shell.execute_reply":"2023-05-07T19:25:10.359090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload data\n# PCA_transformer = PCA(n_components=4)\n# x_train_reshaped_TSE = np.asarray(x_te_latent_recons, dtype='float64')\n# PCA_representation = PCA_transformer.fit_transform(x_train_reshaped_TSE)\n\n# Visualize results\n# plot_representation(PCA_representation, y_te, 'PCA')","metadata":{"_uuid":"fbb356c0-a7d1-4676-9d32-ed6d02c9ff8e","_cell_guid":"2779ce44-7d48-4162-a347-2ee3096bc3ab","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:10.362929Z","iopub.execute_input":"2023-05-07T19:25:10.363213Z","iopub.status.idle":"2023-05-07T19:25:10.368991Z","shell.execute_reply.started":"2023-05-07T19:25:10.363187Z","shell.execute_reply":"2023-05-07T19:25:10.368122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.4 - Visualização da estimativa de densidade do kernel do domínio de origem\n##### 9.3.4 - Visualization of the Kernel Density Estimation of the source domain","metadata":{"_uuid":"d9582cc8-3b06-4b33-b82f-e069c1b5bc36","_cell_guid":"46ede288-dd71-4168-b1e0-04d6030e2e79","trusted":true}},{"cell_type":"code","source":"###\nis_STI_1 =  dataset['STI'] == 1\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.1')\nplt.show()\n###\n###\nis_STI_1 =  dataset['STI'] == 2\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.2')\nplt.show()\n###\nis_STI_1 =  dataset['STI'] == 3\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.3')\nplt.show()\n###\nis_STI_1 =  dataset['STI'] == 4\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.4')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 5\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.5')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 6\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.6')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 7\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.7')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 8\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.8')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 9\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.9')\nplt.show()","metadata":{"_uuid":"11303cf7-26ef-452e-bccc-efc596986a36","_cell_guid":"d04ff949-b5c9-4ec6-a033-c3894d793a68","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:10.370723Z","iopub.execute_input":"2023-05-07T19:25:10.371062Z","iopub.status.idle":"2023-05-07T19:25:12.254182Z","shell.execute_reply.started":"2023-05-07T19:25:10.371027Z","shell.execute_reply":"2023-05-07T19:25:12.253308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.5 -  Mapeamento do conjunto de origem\n##### 9.3.5 - - Source Domain Mapping","metadata":{"_uuid":"0357d64e-ac4f-484f-a8f1-500b84e18d80","_cell_guid":"ae6aba4a-0a61-4f0d-8e58-1959fa09a664","trusted":true}},{"cell_type":"code","source":"z_mapping_VAE_source_domain = dataset\nz_mapping_VAE_source_domain","metadata":{"_uuid":"087ec02c-9fe4-4be7-ba0c-dc6e135fceca","_cell_guid":"d6d47483-c1c5-47ef-b304-9aaa91fb07bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:12.255532Z","iopub.execute_input":"2023-05-07T19:25:12.256045Z","iopub.status.idle":"2023-05-07T19:25:12.270828Z","shell.execute_reply.started":"2023-05-07T19:25:12.256006Z","shell.execute_reply":"2023-05-07T19:25:12.269721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3.6 - Visualização 2D do KDE do espaço codificado bidimensional e valores STI para os domínios de origem-destino\n##### 9.3.6 - 2D KDE Visualization of the Bidimensional Encoded Space and STI Values for the Source-Target Domains","metadata":{"_uuid":"b3120063-62d7-47c1-aa0c-ef8b032ce8d0","_cell_guid":"b555a14f-97c9-415d-8f85-18ee9431eb82","trusted":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import kde\n \n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 1\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z1')\nplt.title('STI = 0.1')\nplt.show()\n\n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 3\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z1')\nplt.title('STI = 0.3')\nplt.show()\n\n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 6\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z1')\nplt.title('STI = 0.6')\nplt.show()\n#\n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 9\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z1')\nplt.title('STI = 0.9')\nplt.show()\n\n# Change color palette\n#plt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto', cmap=plt.cm.Greens_r)\n#plt.show()","metadata":{"_uuid":"3bdbe130-98bb-49c2-83b1-4969f8935307","_cell_guid":"2a833db3-21f8-4a7f-b275-708fbad9f06d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:12.272397Z","iopub.execute_input":"2023-05-07T19:25:12.272817Z","iopub.status.idle":"2023-05-07T19:25:22.429430Z","shell.execute_reply.started":"2023-05-07T19:25:12.272780Z","shell.execute_reply":"2023-05-07T19:25:22.428561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10 - KCPR e outros métodos de regressão multivariada \n### 10 - KCPR and Other Multivariate Regression Methods  \n#### Ridge Regression, PCR, KCPR, and PLSR: Multivariate Regression Methods","metadata":{"_uuid":"0f44f128-01c1-4c06-8621-0cf970789769","_cell_guid":"faa66100-7b92-44d4-a095-1aa9901cb515","trusted":true}},{"cell_type":"markdown","source":"## 10.1 - Rigde Regression","metadata":{"_uuid":"18663d97-c11b-4c79-b9dc-9b8685ed002f","_cell_guid":"54e7c3c6-f5cd-4d52-b86c-2380ea3a07af","trusted":true}},{"cell_type":"code","source":"from numpy import arange\nfrom pandas import read_csv\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge\n\n# load the dataset\nX, y = dataset[['z1','z2']].values, dataset['STI'].values\n\n# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n\n# define search\nsearch = GridSearchCV(model, grid, \n                      scoring='neg_mean_absolute_error', \n                      cv=cv, \n                      n_jobs=-1)\n\n# perform the search\nresults = search.fit(X, y)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","metadata":{"_uuid":"dd30e1fc-eec8-49d7-b5ec-84520543d901","_cell_guid":"c9a617cf-cddc-4771-9417-2c86a53302fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:22.430695Z","iopub.execute_input":"2023-05-07T19:25:22.431204Z","iopub.status.idle":"2023-05-07T19:25:31.525752Z","shell.execute_reply.started":"2023-05-07T19:25:22.431163Z","shell.execute_reply":"2023-05-07T19:25:31.524196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.2 - PCR - KCPR - PLSR","metadata":{"_uuid":"40f384f6-4d1b-4a4c-843c-5ca8c7a54b34","_cell_guid":"75f578cb-e44c-492f-9a75-3254e3167d7f","trusted":true}},{"cell_type":"code","source":"# Since we are using unbalaced data (STI values), we must first balance out the data\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\nover_sampler = RandomOverSampler(random_state=42)\nX_res, y_res = over_sampler.fit_resample(X, y)\n\nprint(f\"Training target statistics: {Counter(y_res)}\")\nprint(f\"Testing target statistics: {Counter(y_test)}\")","metadata":{"_uuid":"67df9adb-f63f-4cb7-a9c8-0dee1a13bd52","_cell_guid":"ed18f5b4-3ae2-42ed-a178-c6c0c9e6ae82","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:31.532616Z","iopub.execute_input":"2023-05-07T19:25:31.535912Z","iopub.status.idle":"2023-05-07T19:25:31.661624Z","shell.execute_reply.started":"2023-05-07T19:25:31.535844Z","shell.execute_reply":"2023-05-07T19:25:31.660683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.3 - Comparação e Benchmarking de Modelos de Regressão Multivariada\n#### 10.3 - Comparison and Benchmarking of Multivariate Regression Models","metadata":{"_uuid":"c8970113-dbbb-4a5f-902d-071c496f3aaa","_cell_guid":"c196702d-f566-4a9a-abd7-02630c8121a4","trusted":true}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.decomposition import KernelPCA \n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\nover_sampler = RandomOverSampler(random_state=42)\n\nX, y = dataset[['z1','z2']].values, dataset['STI'].values\nX_res, y_res = over_sampler.fit_resample(X, y)\n\nprint(f\"Training target statistics: {Counter(y_res)}\")\nprint(f\"Testing target statistics: {Counter(y_test)}\")\n\n# X, y = dataset[['z1','z2']].values, dataset['STI'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\npcr = make_pipeline(StandardScaler(), \n                    PCA(n_components=2), \n                    LinearRegression())\n\npcr.fit(X_train, y_train)\n\npca = pcr.named_steps[\"pca\"]  # retrieve the PCA step of the pipeline\n\nkpcr = make_pipeline(StandardScaler(), \n                     KernelPCA(kernel=\"rbf\", n_components=2),\n                     LinearRegression()\n                    )\n\nkpcr.fit(X_train, y_train)\n# kpca = kpcr.named_steps[\"KernelPCA\"]  # retrieve the PCA step of the pipeline\n\npls = PLSRegression(n_components=1)\npls.fit(X_train, y_train)\n\nsvr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\nsvr.fit(X_train, y_train)\n\nfig, axes = plt.subplots(1, 4, figsize=(10, 3))\n\naxes[0].scatter(\n    y_test, pcr.predict(X_test), alpha=0.3, label=\"predictions\"\n)\naxes[0].set(xlabel=\"STI augmented\", ylabel=\"y\", title=\"PCR\")\n\naxes[1].scatter(\n    y_test, pls.predict(X_test), alpha=0.3, label=\"predictions\"\n)\n\naxes[1].set(xlabel=\"STI augmented\", ylabel=\"y\", title=\"PLSR\")\n\naxes[2].scatter(\n    y_test, kpcr.predict(X_test), alpha=0.3, label=\"predictions\"\n)\naxes[2].set(xlabel=\"STI augmented\", ylabel=\"y\", title=\"KPCR\")\n\naxes[3].scatter(\n    y_test, svr.predict(X_test), alpha=0.3, label=\"predictions\"\n)\naxes[3].set(xlabel=\"STI augmented\", ylabel=\"y\", title=\"SVR\")\n\nfor ax in axes.flat: #this will iterate over all 6 axes\n    ax.set_xlim(-1, 10)\n    ax.set_ylim(-1, 10) \n    \nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"f1b47d56-8395-445b-bffd-fe202205ca25","_cell_guid":"7dfa8d78-f6db-4967-a0d6-7630c60259d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:25:31.662876Z","iopub.execute_input":"2023-05-07T19:25:31.663376Z","iopub.status.idle":"2023-05-07T19:26:18.984172Z","shell.execute_reply.started":"2023-05-07T19:25:31.663338Z","shell.execute_reply":"2023-05-07T19:26:18.983421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.4 - Modelagem da Regressão de Kernel (KCPR)\n## 10.4 - KCPR Modeling","metadata":{"_uuid":"fe91db32-c3e3-4504-b1e7-64114b36e269","_cell_guid":"bbb3d372-1328-44c4-b34f-18fb421ec392","trusted":true}},{"cell_type":"code","source":"from sklearn.gaussian_process.kernels import ConstantKernel, RBF, DotProduct\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom numpy import linalg as LA\n\n# https://web.mit.edu/modernml/course/lectures/MLClassLecture3.pdf (Pag. 5)\n# kernel = RBF()\n# RBF_KERNEL = kernel.__call__(X)\n# kernel = ConstantKernel()\n# kernel.__call__(X)\n# kernel = DotProduct()\n# kernel.__call__(X)\n\n# Load the dataset\ndf_KPCR  = pd.read_csv('/kaggle/input/sti-prediction/dataset_high_z_dim_to_KCPR.csv')\ndf_KPCR.dropna(inplace=True)\n\n# center the values in each column of the DataFrame\n# X = df[['z1','z2']].values\n# y = df['STI'].values\n\ndf_KPCR = df_KPCR.groupby(by=[\"classes\"], dropna=True).mean()\ndf_KPCR = df_KPCR.drop(columns=[\"Unnamed: 0\"])\n\n# Mean centraing \ndf_KPCR = df_KPCR.apply(lambda x: x-x.mean())\n\nX_KPCR = df_KPCR.values\ny_KPCR = df_KPCR.index.values\n\nax = df_KPCR.T.plot.kde(bw_method=0.3)\nax.set_xlim([-.2, 0.2])\nplt.show()\n\ny_KPCR\nX_KPCR.shape\nX_KPCR\n\n# Get the kernel and calculte the respective transformation\nkernel = RBF(0.2)\nK = kernel.__call__(X_KPCR)\n\n# centralize the kenerl\n'''\nN = K.shape[0]\none_n = np.ones((N,N)) / N\nK = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n) \n'''\n\n# Verify if the kernel is centralized by the mean\n# print(K.mean(axis=1))\n\n# Calcuting the eigedecompostion\n# eigenValues, eigenVectors = LA.eig(np.transpose(K) * K)\neigenValues, eigenVectors = LA.eig(K)\n\n# Verirify if the kernel othornomal\n# abs(eigenVectors)\n\nZ = K  * eigenVectors\n# Z = K  * abs(eigenVectors)\n\ngamma = LA.inv(np.transpose(Z) * Z) * np.transpose(Z) * y_KPCR\ny_pred = np.diag(Z*gamma)\n\nplt.scatter(y_KPCR, y_pred)\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z2')\nplt.legend(title=\"y = KPCR(X): Curva de Calibração\")\nplt.show()\nplt.show()\n\n#M aking the model to predict function\n# https://web.mit.edu/modernml/course/lectures/MLClassLecture3.pdf\n\nx_hat = X_KPCR[5]\ny_hat = (np.transpose(np.expand_dims(y_KPCR, 1)) *  K \\\n         * kernel.__call__(X_KPCR, np.transpose(np.expand_dims(x_hat, 1)))).max()\ny_hat","metadata":{"_uuid":"54fc5b3a-e655-44a5-acef-f69889f08eff","_cell_guid":"4d66cc4f-4be9-4abb-aafa-51c3546b8e96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:18.985492Z","iopub.execute_input":"2023-05-07T19:26:18.985834Z","iopub.status.idle":"2023-05-07T19:26:20.606580Z","shell.execute_reply.started":"2023-05-07T19:26:18.985806Z","shell.execute_reply":"2023-05-07T19:26:20.605650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.5 - Persistência do modelo KCPR e SVR para ser usada como uma função de perda personalizada no modelo MVPAnP\n#### 10.5 - KCPR and SVR Model persistence to be used as a custom loss function in the model MVPAnP Model","metadata":{"_uuid":"5336cfef-982d-4355-b879-3d5671680003","_cell_guid":"167da091-def0-433a-be5e-c841789dc0c9","trusted":true}},{"cell_type":"code","source":"from joblib import dump, load\ndump(svr, 'srv_reg.joblib') \n\nsvr_persited  = load('srv_reg.joblib') \n\nsvr_persited.predict(X_res[0:1])","metadata":{"_uuid":"db472883-0feb-4c71-a15c-ebb89fa6f8ae","_cell_guid":"690b9c59-5a8f-4b27-8a0d-615b9605e5e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:20.607911Z","iopub.execute_input":"2023-05-07T19:26:20.608277Z","iopub.status.idle":"2023-05-07T19:26:20.621200Z","shell.execute_reply.started":"2023-05-07T19:26:20.608228Z","shell.execute_reply":"2023-05-07T19:26:20.620312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11 - Metodologia de Transferência de Aprendizagem\n### 11 - Transfer Learning Methodology","metadata":{"_uuid":"c3cc67a7-a33c-4a33-9f15-31fcf6b50c42","_cell_guid":"590baa29-f5c1-4551-ad6a-001181be90a0","trusted":true}},{"cell_type":"markdown","source":"## 11.1 - Carregar os dados do domínio de destino\n#### 11.1 - Load the Target Domain Data","metadata":{"_uuid":"9f4aa962-1e06-4120-b51a-88127c135d74","_cell_guid":"2b94d613-b82e-4bb1-9d18-c22a42798da4","trusted":true}},{"cell_type":"code","source":"## Loader function of test/validation dataset for transfer learning\n\n# Load data\nA = np.load('../input/sti-prediction/data_to_transfer_02_11_22.npz')\nA.files\nA = A['arr_0']\ndf = pd.DataFrame({'c1': A[:,0], 'c2': A[:,1], 'c3': A[:,2], \\\n                   'c4': A[:,3], 'c5': A[:,4]})\n\n\"\"\"## 2 - Functions settings\n\"\"\"\n\ndef feature_normalize(dataset):\n\n    mu = np.mean(dataset, axis=0)\n    sigma = np.std(dataset, axis=0)\n    return (dataset - mu)/sigma\n\n\ndef show_confusion_matrix(validations, predictions):\n\n    matrix = metrics.confusion_matrix(validations, predictions)\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(matrix,\n                cmap=\"coolwarm\",\n                linecolor='white',\n                linewidths=1,\n                xticklabels=LABELS,\n                yticklabels=LABELS,\n                annot=True,\n                fmt=\"d\")\n    \n    plt.title(\"Confusion Matrix\")\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.show()\n\n\ndef show_basic_dataframe_info(dataframe,\n                              preview_rows=20):\n\n    \"\"\"\n    This function shows basic information for the given dataframe\n    Args:\n        dataframe: A Pandas DataFrame expected to contain data\n        preview_rows: An integer value of how many rows to preview\n    Returns:\n        Nothing\n    \"\"\"\n\n    # Shape and how many rows and columns\n    print(\"Number of columns in the dataframe: %i\" % (dataframe.shape[1]))\n    print(\"Number of rows in the dataframe: %i\\n\" % (dataframe.shape[0]))\n    print(\"First 20 rows of the dataframe:\\n\")\n    # Show first 20 rows\n    print(dataframe.head(preview_rows))\n    print(\"\\nDescription of dataframe:\\n\")\n    # Describe dataset like mean, min, max, etc.\n    # print(dataframe.describe())\n\n\ndef read_data(file_path):\n\n    \"\"\"\n    This function reads the data from a file\n    Args:\n        file_path: URL pointing to the pickle file\n    Returns:\n        A pandas dataframe\n    \"\"\"\n\n    df.rename(columns = {'c1': 'Room-id', 'c2': 'STI', \n                         'c3': 'Freq - [Hz]', 'c4': 'PSD(RIR)', \n                         'c5': 'PSD(BGN)' },  inplace = True)\n   \n    # This is very important otherwise the model will not fit and loss\n    # will show up as NAN\n    \n    label_encoder = LabelEncoder()\n    n_bins = 10 # number of classes\n\n   # 10 number of classes accoring to the STI range (0,1)\n    y = label_encoder.fit_transform(pd.cut(df['STI'], n_bins, retbins=True)[0])\n\n    df['STI'] = y   \n    df.dropna(axis=0, how='any', inplace=True)\n\n    return df\n\n\ndef convert_to_float(x):\n\n    try:\n        return np.float(x)\n    except:\n        return np.nan\n\n\n# Not used right now\ndef feature_normalize(dataset):\n\n    mu = np.mean(dataset, axis=0)\n    sigma = np.std(dataset, axis=0)\n    return (dataset - mu)/sigma\n\n\ndef plot_axis(ax, x, y, title):\n\n    ax.plot(x, y)\n    ax.set_title(title)\n    ax.xaxis.set_visible(False)\n    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n    ax.set_xlim([min(x), max(x)])\n    ax.grid(True)\n\n\ndef plot_activity(activity, data):\n\n    fig, (ax0, ax1) = plt.subplots(nrows=2,\n         figsize=(15, 10),\n         sharex=True)\n    plot_axis(ax0, data['Freq - [Hz]'], data['PSD(RIR)'], 'PSD(RIR)')\n    plot_axis(ax1, data['Freq - [Hz]'], data['PSD(BGN)'], 'PSD(BGN)')\n    plt.subplots_adjust(hspace=0.2)\n    fig.suptitle(activity)\n    plt.subplots_adjust(top=0.90)\n    #plt.show()\n\n\ndef create_segments_and_labels(df, time_steps, step, label_name):\n\n    \"\"\"\n    This function receives a dataframe and returns the reshaped segments\n    of BGN, RIR data as well as the corresponding STI labels\n    Args:\n        df: Dataframe in the expected format\n        time_steps: Integer value of the length of a segment that is created\n    Returns:\n        reshaped_segments\n        labels:\n    \"\"\"\n\n    N_FEATURES = 2\n    \n    # Number of steps to advance in each iteration (for me, it should always\n    # be equal to the time_steps in order to have no overlap between segments)\n    # step = time_steps\n    \n    segments = []\n    labels = []\n    \n    for i in range(0, len(df) - time_steps, step):\n        xs = df['PSD(RIR)'].values[i: i + time_steps]\n        ys = df['PSD(BGN)'].values[i: i + time_steps]\n        xs = xs[1:]\n        ys = ys[1:]\n        # Retrieve the most often used label in this segment\n        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n        segments.append([xs, ys])\n        labels.append(label)\n\n    # Bring the segments into a better shape\n    reshaped_segments = np.asarray(segments, dtype= np.float32) \\\n                          .reshape(-1, time_steps-1, N_FEATURES)\n    labels = np.asarray(labels)\n\n    return reshaped_segments, labels\n\n\"\"\"\n\n## Create training database\n\"\"\"\n\n# Set some standard parameters upfront\npd.options.display.float_format = '{:.1f}'.format\n# sns.set() # Default seaborn look and feel\n# plt.style.use('ggplot')\nprint('keras version ', keras.__version__)\n\nLABELS = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6 \",\"7\",\"8\",\"9\",\"10\"]\n\n# The number of steps within one time segment\nTIME_PERIODS = len(np.arange(0,8001,10))\nSTEP_DISTANCE = TIME_PERIODS\n\nprint(\"\\n--- Load, inspect and transform data ---\\n\")\n\nprint(\"Load data set from the npz\")\nread_data(df)\n# Describe the data\nshow_basic_dataframe_info(df, 20)\n\n# Total number of rooms\nprint(\"\\n--- Calculating total number of classrooms ---\\n\")\nnum_rooms = len(df) / TIME_PERIODS\n\ndf['STI'].value_counts().plot(kind='bar', title='Training Examples by STI classes')\nplt.savefig('STI_distribution_target_domain.pdf')\n#plt.show()\n\nprint(\"\\n--- Reshape the data into segments ---\\n\")\n\n# Differentiate between test set and training set\ndf_train = df[df['Room-id'] <= 0.7* num_rooms]\ndf_test = df[df['Room-id'] > 0.7* num_rooms]\n\n# Normalize features for training data set\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ncolumn_names_to_normalize = ['PSD(RIR)',  'PSD(BGN)']\n\nx = df_train[column_names_to_normalize].values\nx_scaled = scaler.fit_transform(x)\n\ndf_temp = pd.DataFrame(x_scaled, \n                       columns=column_names_to_normalize, \n                       index = df_train.index)\ndf_train[column_names_to_normalize] = df_temp\n\n# Test dataset\nx = df_test[column_names_to_normalize].values\nx_scaled = scaler.fit_transform(x)\n\ndf_temp = pd.DataFrame(x_scaled, \n                       columns=column_names_to_normalize, \n                       index = df_test.index)\n\ndf_test[column_names_to_normalize] = df_temp\n\n# Round in order to comply to NSNumber\ndf_train = df_train.round({'PSD(RIR)': 6, 'PSD(BGN)': 6})\ndf_test =  df_test.round({'PSD(RIR)': 6, 'PSD(BGN)': 6})\n\n# Reshape the training data into segments, so that they can be processed by \n# the network\n# Define column name of the label vector\n\nLABEL = \"STI\"\nx_train, y_train = create_segments_and_labels(df_train,\n                                              TIME_PERIODS,\n                                              STEP_DISTANCE,\n                                              LABEL)\n\nx_test, y_test = create_segments_and_labels(df_test,\n                                            TIME_PERIODS,\n                                            STEP_DISTANCE,\n                                            LABEL)","metadata":{"_uuid":"b1a6127e-08fa-4b19-826a-084da4facc8a","_cell_guid":"38cb1d36-9019-4624-833e-f20fce01d650","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:20.623031Z","iopub.execute_input":"2023-05-07T19:26:20.623304Z","iopub.status.idle":"2023-05-07T19:26:22.616271Z","shell.execute_reply.started":"2023-05-07T19:26:20.623277Z","shell.execute_reply":"2023-05-07T19:26:22.615344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"\\n--- Reshape data to be accepted by Keras ---\\n\")\n\n# Inspect x data\nprint('x_train shape: ', x_train.shape)\nprint(x_train.shape[0], 'training samples')\n\n# Inspect y data\nprint('y_train shape: ', y_train.shape)\n\nprint('y_test shape: ', y_test.shape)\n\n# Set input_shape / reshape for Keras\n\nprint(\" Verifying the trainign dataset dimensions\")\n\nprint(\"Dimensoes antes\")\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\n# Input image dimensions\nimg_rows = 1600  \n# len(np.arange(0,8001,10)) aproximação 800 TR and 800 TO BGN\n\n# Channels go last for TensorFlow backend\nprint(\"Dimensão após restruturacao - treino\")\nx_train_reshaped = x_train.reshape(x_train.shape[0], img_rows)\nprint(x_train_reshaped.shape)\n\n# Agora fazer o teste\nprint(\"Dimensão após restruturacao - teste\")\nx_test_reshaped = x_test.reshape(x_test.shape[0], img_rows)\nprint(x_test_reshaped.shape)\n\nx_tr = x_train_reshaped\nx_te = x_test_reshaped","metadata":{"_uuid":"b09ffd04-6c69-430d-bafe-44db2ece6914","_cell_guid":"01e5a1aa-8297-41af-87e0-39985ecab0b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:22.617640Z","iopub.execute_input":"2023-05-07T19:26:22.617975Z","iopub.status.idle":"2023-05-07T19:26:22.631150Z","shell.execute_reply.started":"2023-05-07T19:26:22.617939Z","shell.execute_reply":"2023-05-07T19:26:22.630144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11.2 - Estatísticas de distribuição de dados do domínio de destino\n## 11.2 - Target Domain Data Distribution Statistics","metadata":{"_uuid":"c9e7eab7-2676-4f94-b9d2-a1133dd39595","_cell_guid":"c0b84a2d-01c7-4ef1-9d47-157ea1287c55","trusted":true}},{"cell_type":"code","source":"### Fitting \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Extract the column from the dataframe and convert it to a numpy array\ndata = (df['STI']/10).values\n\n# Fit a Gaussian distribution to the data\nmu, std = stats.norm.fit(data)\n\n# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='r')\n\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"2ef7668e-8f64-4412-9c18-f91322b92987","_cell_guid":"a4a248b3-188a-419e-8213-b443ba15b922","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:22.632535Z","iopub.execute_input":"2023-05-07T19:26:22.632875Z","iopub.status.idle":"2023-05-07T19:26:22.797101Z","shell.execute_reply.started":"2023-05-07T19:26:22.632840Z","shell.execute_reply":"2023-05-07T19:26:22.796191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='r')\n\n# Plot the Gaussian fit\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, mu, std)\n\n# plt.plot(x, c, 'r', linewidth=2)\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\nax2 = plt.twinx()\n\n# plot the data for the second y-axis\nax2.plot(x, p, 'r--')\nax2.set_ylabel('Função densidade acumuldada', color='b')\n\n# show the plot\nplt.show()\n\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"3bdf1eac-3932-4889-9c69-7295279816c8","_cell_guid":"6bfd4ae3-23a6-43ac-a656-b04dabf05d3f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:22.798519Z","iopub.execute_input":"2023-05-07T19:26:22.798843Z","iopub.status.idle":"2023-05-07T19:26:23.024173Z","shell.execute_reply.started":"2023-05-07T19:26:22.798807Z","shell.execute_reply":"2023-05-07T19:26:23.023224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram\nplt.hist(data, bins=20, density=True, alpha=0.6, color='r')\n\n# Plot the Gaussian fit\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\nc = stats.norm.cdf(x, mu, std)\n\n# plt.plot(x, c, 'r', linewidth=2)\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Função densidade de probabilidade')\n\nax2 = plt.twinx()\n\n# plot the data for the second y-axis\nax2.plot(x, c, 'r--')\nax2.set_ylabel('Função densidade acumulada', color='r')\n\n# show the plot\nplt.show()\n\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"937ddd09-4b55-4bf8-b631-6eb0c9f14f27","_cell_guid":"8c034b39-5e93-46fa-8dae-be76a01ea360","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.025564Z","iopub.execute_input":"2023-05-07T19:26:23.026077Z","iopub.status.idle":"2023-05-07T19:26:23.265427Z","shell.execute_reply.started":"2023-05-07T19:26:23.026037Z","shell.execute_reply":"2023-05-07T19:26:23.264577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Fitting \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\n# Extract the column from the dataframe and convert it to a numpy array\ndata = df['STI'].values\n\n# Fit a Gaussian distribution to the data\nmu, std = stats.norm.fit(data)\n\n# Plot the histogram\nplt.hist(data, bins=25, density=True, alpha=0.6, color='b')\n\n# Plot the Gaussian fit\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = stats.norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Add a title and labels\nplt.xlabel('STI')\nplt.ylabel('Probability Density')\n\n# Show the plot\nprint(mu, std)","metadata":{"_uuid":"a6de09a7-74f3-4dd3-8cb6-d0677509ff05","_cell_guid":"548ad252-b8d5-4789-ada0-4869b4535588","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.267076Z","iopub.execute_input":"2023-05-07T19:26:23.267539Z","iopub.status.idle":"2023-05-07T19:26:23.441977Z","shell.execute_reply.started":"2023-05-07T19:26:23.267503Z","shell.execute_reply":"2023-05-07T19:26:23.441034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_tr.shape","metadata":{"_uuid":"06d78768-d057-44f2-a668-4efab7813206","_cell_guid":"038980b8-35ba-4278-beab-c44e8ee49da4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.443353Z","iopub.execute_input":"2023-05-07T19:26:23.443701Z","iopub.status.idle":"2023-05-07T19:26:23.451011Z","shell.execute_reply.started":"2023-05-07T19:26:23.443664Z","shell.execute_reply":"2023-05-07T19:26:23.450013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"_uuid":"2896e957-2f0d-4a25-8647-3bc9f3012bc8","_cell_guid":"9c0652ca-6213-444e-87e7-d47b9c4b0f90","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.452400Z","iopub.execute_input":"2023-05-07T19:26:23.453117Z","iopub.status.idle":"2023-05-07T19:26:23.461155Z","shell.execute_reply.started":"2023-05-07T19:26:23.453078Z","shell.execute_reply":"2023-05-07T19:26:23.460153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12 - Reconstruindo o espaço latente VAE para o domínio de destino por meio do modelo de codificador treinado no domínio de origem\n### 12 - Reconstructing the VAE Latent Space for the Target Domain via the Encoder Model Trained on the Source Domain","metadata":{"_uuid":"5e05eff9-c6cd-4798-a4e8-c5b8d9872a00","_cell_guid":"d5a40c81-9906-4cf2-b952-91c5264d0758","trusted":true}},{"cell_type":"code","source":"reconstructed_model = keras.models.load_model(\"vae_encoder.h5\")\n\n# x_tr_latent = encoder.predict(x_tr, batch_size=batch_size)\nx_tr_latent = reconstructed_model.predict(x_tr, batch_size=1)\ny_tr = y_train                            \ndataset = pd.DataFrame({'z1': x_tr_latent[:, 0], 'z2': x_tr_latent[:, 1], 'STI':  y_tr})\nprint(dataset)","metadata":{"_uuid":"210a668a-dbb9-4da7-84eb-87703e9c17c5","_cell_guid":"c24b31eb-6dd3-4b52-80fc-9bd5824c9e94","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.463005Z","iopub.execute_input":"2023-05-07T19:26:23.463541Z","iopub.status.idle":"2023-05-07T19:26:23.821829Z","shell.execute_reply.started":"2023-05-07T19:26:23.463505Z","shell.execute_reply":"2023-05-07T19:26:23.820887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12.1 - Plotando a representação do espaço latente (z = 2) dos dados de teste no conjunto de dados de destino\n### 12.1 - Plotting the Latent Space Representation (z = 2) of the Testing Data in the Target Dataset","metadata":{"_uuid":"04fbfd40-464a-450b-bb12-b581b9e3a9c7","_cell_guid":"b52e3257-a9f9-4f86-9ed6-643145c94b29","trusted":true}},{"cell_type":"code","source":"# Latent Space obtined from the target domain\n# Plot the target domain (target)\n\nplt.figure(figsize=(6, 6))\ny_tr = y_train\nplt.scatter(x_tr_latent[:, 0], x_tr_latent[:, 1], c=y_tr/10)\nplt.colorbar()\n\nplt.legend(title=\"STI - Dados de treinamento (Alvo) \")\nplt.xlabel('Dimensão - z1')\nplt.ylabel('Dimensão - z2')\n#ax.add_artist(legend1)\n\nplt.show()","metadata":{"_uuid":"4f09b558-8c74-4ef4-b102-113d98faa841","_cell_guid":"0a428241-a814-487d-8a92-bbae3a5b5916","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:23.823212Z","iopub.execute_input":"2023-05-07T19:26:23.823751Z","iopub.status.idle":"2023-05-07T19:26:24.055324Z","shell.execute_reply.started":"2023-05-07T19:26:23.823712Z","shell.execute_reply":"2023-05-07T19:26:24.054408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 12.1.1 - Visualização da Estimativa de Densidade do Kernel (KDE) para o Domínio de Destino\n##### 12.1.1 - Visualization of Kernel Density Estimation (KDE) for the Target Domain","metadata":{"_uuid":"92559937-79ff-40ed-8ab4-bef2bf3be8d9","_cell_guid":"d4ea8aeb-b370-455a-bbda-4cadcccaa6e5","trusted":true}},{"cell_type":"code","source":"###\nis_STI_1 =  dataset['STI'] == 1\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.1')\nplt.show()\n###\n###\nis_STI_1 =  dataset['STI'] == 2\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.2')\nplt.show()\n###\nis_STI_1 =  dataset['STI'] == 3\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.3')\nplt.show()\n###\nis_STI_1 =  dataset['STI'] == 4\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.4')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 5\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.5')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 6\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.6')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 7\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.7')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 8\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.8')\nplt.show()\n####\nis_STI_1 =  dataset['STI'] == 9\nis_STI_1_data = dataset[is_STI_1]\n#print(is_STI_1_data)\nis_STI_1_data = is_STI_1_data[['z1','z2']]\nax = is_STI_1_data.plot.kde()\nplt.xlabel('Dimensão latente - z')\nplt.ylabel('Densidade de Kernel')\nplt.title('STI = 0.9')\nplt.show()","metadata":{"_uuid":"a86f315d-fadd-410e-bbd1-ddb60a98aea5","_cell_guid":"4806b839-846a-44f4-be69-b8a08ee39afe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:24.057067Z","iopub.execute_input":"2023-05-07T19:26:24.057408Z","iopub.status.idle":"2023-05-07T19:26:25.484182Z","shell.execute_reply.started":"2023-05-07T19:26:24.057373Z","shell.execute_reply":"2023-05-07T19:26:25.483337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 12.1.1.1 - MAPEAMENTO DOS DADOS DE ALVO (MMD - Target Domain)","metadata":{"_uuid":"65f57f29-e506-4c22-9ec6-bd9822243440","_cell_guid":"aa5accf9-4e7e-473b-b62f-fee949e4150c","trusted":true}},{"cell_type":"code","source":"z_mapping_VAE_target_domain = dataset\nz_mapping_VAE_target_domain","metadata":{"_uuid":"1d2f17b1-a9cf-4e12-ad7d-a3825176d75d","_cell_guid":"0d2c3901-50a3-4cbf-a0b0-db4a77b29934","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:25.485511Z","iopub.execute_input":"2023-05-07T19:26:25.485837Z","iopub.status.idle":"2023-05-07T19:26:25.498455Z","shell.execute_reply.started":"2023-05-07T19:26:25.485801Z","shell.execute_reply":"2023-05-07T19:26:25.497345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 12.1.2 - KDE 2D Visualization of the bidmensional encoded space and STI values (Target Domain)","metadata":{"_uuid":"f65e7ec0-b193-4039-8304-0401a1c4b38c","_cell_guid":"a87ba990-5ff4-49d0-9325-fc683c562b17","trusted":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import kde\n \n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 1\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z2')\nplt.title('STI = 0.1')\nplt.show()\n#####\n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 3\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z2')\nplt.title('STI = 0.3')\nplt.show()\n#####\n######## create data\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 6\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z2')\nplt.title('STI = 0.6')\nplt.show()\n\n######## create data\n'''\nplt.figure(figsize=(6, 6))\nis_STI_1 =  dataset['STI'] == 9\nis_STI_1_data = dataset[is_STI_1]\n\nx = is_STI_1_data['z1']\ny = is_STI_1_data['z2']\n \n# Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents\nnbins=300\nk = kde.gaussian_kde([x,y])\nxi, yi = np.mgrid[x.min():x.max():nbins*1j, y.min():y.max():nbins*1j]\nzi = k(np.vstack([xi.flatten(), yi.flatten()]))\n \n# Make the plot\nplt.pcolormesh(xi, yi, zi.reshape(xi.shape), shading='auto')\n\nplt.xlabel('Dimensão latente - z1')\nplt.ylabel('Dimensão latente - z2')\nplt.title('STI = 0.9')\nplt.show()\n'''","metadata":{"_uuid":"30a6e6c2-b01d-4a99-81bd-d6e4dea10ce6","_cell_guid":"06a46c92-6e78-41a5-a563-f55c0adcdd9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:25.499731Z","iopub.execute_input":"2023-05-07T19:26:25.500049Z","iopub.status.idle":"2023-05-07T19:26:26.482534Z","shell.execute_reply.started":"2023-05-07T19:26:25.500016Z","shell.execute_reply":"2023-05-07T19:26:26.481579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12.2 - Cálculo da métrica MMD e visualização\n### 12.2 - MMD metric calculation and visualization of the Maximum Mean Discrepancy (MMD)","metadata":{"_uuid":"d64bfff0-517f-43fe-ad0f-a065292dc0a5","_cell_guid":"6423cf84-4bec-4fee-8de4-5f2b5e72048f","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import dirichlet\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nm = 20 # sample size, a smal number for the opimzation\n\nx_mean = torch.FloatTensor(([z_mapping_VAE_source_domain[\"z1\"].mean(),\n               z_mapping_VAE_source_domain[\"z2\"].mean()])) \n\n#t orch.zeros(2) + 1 #(pegue as médias z1 e z2 form Source)\n\ny_mean = torch.FloatTensor(([z_mapping_VAE_target_domain[\"z1\"].mean(),\n               z_mapping_VAE_target_domain[\"z2\"].mean()])) \n\n#torch.zeros(2)     #(pegue as médias z1 e z2 form Target)\n\n# I can calculate the both\nx_cov =  torch.FloatTensor(\n                           np.cov([z_mapping_VAE_source_domain[\"z1\"], \n                                  z_mapping_VAE_source_domain[\"z2\"]])\n                                  ) * torch.eye(2) #2*torch.eye(2) \n                          \n# IMPORTANT: Covariance matrices must be positive definite\ny_cov = torch.FloatTensor(\n                          np.cov([z_mapping_VAE_target_domain[\"z1\"], \n                                 z_mapping_VAE_target_domain[\"z2\"]])\n                                 ) * torch.eye(2)\n\n# Making the sampling\npx = MultivariateNormal(x_mean, x_cov)\nqy = MultivariateNormal(y_mean, y_cov)\n\nx = px.sample([m]).to(device)\ny = qy.sample([m]).to(device)\n\n\ndef MMD(x, y, kernel):\n    \"\"\"Emprical maximum mean discrepancy. The lower the result\n       the more evidence that distributions are the same.\n\n    Args:\n        x: first sample, distribution P\n        y: second sample, distribution Q\n        kernel: kernel type such as \"multiscale\" or \"rbf\"\n    \"\"\"\n    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()),torch.mm(x, y.t())\n            \n    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n   \n    dxx = rx.t() + rx - 2. * xx \n    dyy = ry.t() + ry - 2. * yy \n    dxy = rx.t() + ry - 2. * zz\n   \n    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device),\n                  torch.zeros(xx.shape).to(device))\n   \n    if kernel == \"multiscale\":\n       \n        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n        for a in bandwidth_range:\n            XX += a**2 * (a**2 + dxx)**-1\n            YY += a**2 * (a**2 + dyy)**-1\n            XY += a**2 * (a**2 + dxy)**-1\n           \n    if kernel == \"rbf\":\n     \n        bandwidth_range = [10, 15, 20, 50]\n        for a in bandwidth_range:\n            XX += torch.exp(-0.5*dxx/a)\n            YY += torch.exp(-0.5*dyy/a)\n            XY += torch.exp(-0.5*dxy/a)\n     \n    return torch.mean(XX + YY - 2. * XY)\n\n\nresult = MMD(x, y, kernel=\"multiscale\")\n\nprint(f\"MMD result of X and Y is {result.item()}\")\n\n# ---- Plotting setup ----\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(8,4), dpi=100)\n# plt.tight_layout()\ndelta = 0.025\n\nx1_val = np.linspace(-5, 5, num=m)\nx2_val = np.linspace(-5, 5, num=m)\n\nx1, x2 = np.meshgrid(x1_val, x2_val)\n\npx_grid = torch.zeros(m,m)\nqy_grid = torch.zeros(m,m)\n\nfor i in range(m):\n    for j in range(m):\n        px_grid[i,j] = multivariate_normal.pdf([x1_val[i],x2_val[j]], \n                                               x_mean, \n                                               x_cov)\n        \n        qy_grid[i,j] = multivariate_normal.pdf([x1_val[i],x2_val[j]], \n                                               y_mean, \n                                               y_cov)\n\n\nCS1 = ax1.contourf(x1, x2, px_grid,100, cmap=plt.cm.YlGnBu)\nax1.set_title(\"Distribution of $X \\sim P(X)$\")\nax1.set_ylabel('$x_2$')\nax1.set_xlabel('$x_1$')\nax1.set_aspect('equal')\nax1.scatter(x[:10,0].cpu(), x[:10,1].cpu(), \n            label=\"$X$ Samples\", \n            marker=\"o\", \n            facecolor=\"r\", \n            edgecolor=\"k\")\n\nax1.legend()\n\nCS2 = ax2.contourf(x1, x2, qy_grid,100, cmap=plt.cm.YlGnBu)\nax2.set_title(\"Distribution of $Y \\sim Q(Y)$\")\nax2.set_xlabel('$y_1$')\nax2.set_ylabel('$y_2$')\nax2.set_aspect('equal')\nax2.scatter(y[:10,0].cpu(), y[:10,1].cpu(), \n            label=\"$Y$ Samples\", \n            marker=\"o\", \n            facecolor=\"r\", \n            edgecolor=\"k\")\nax2.legend()\n\n# ax1.axis([-2.5, 2.5, -2.5, 2.5])\n\n# Add colorbar and title\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\ncbar = fig.colorbar(CS2, cax=cbar_ax)\ncbar.ax.set_ylabel('Density results')\nplt.suptitle(f\"MMD result: {round(result.item(),3)}\",\n             y=0.95, \n             fontweight=\"bold\")\n\nplt.show()","metadata":{"_uuid":"0c4f2f00-7dc9-47d1-ae09-a3caa5da173d","_cell_guid":"28214e09-e3ea-4474-be6e-c4be5b968884","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:26.484073Z","iopub.execute_input":"2023-05-07T19:26:26.484667Z","iopub.status.idle":"2023-05-07T19:26:32.653271Z","shell.execute_reply.started":"2023-05-07T19:26:26.484629Z","shell.execute_reply":"2023-05-07T19:26:32.652355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12.3 - Previsão em lote para o conjunto de dados x_tr\n#### 12.3 - Batch Prediction for the x_tr Dataset","metadata":{"_uuid":"12c826e9-dc5c-48d0-8b8e-049027f4e3d3","_cell_guid":"c67d6b7a-e87d-40f6-b4f4-1929e6c0e601","trusted":true}},{"cell_type":"code","source":"print(x_tr[0].shape)\nx_tr_latent = encoder.predict(x_tr, batch_size=batch_size)\n\n# This result is gonna be the reference value for the Loss function\nsvr.predict(x_tr_latent)\nprint(\"This was the batch prediction\")","metadata":{"_uuid":"19266a55-bf85-4816-b145-0e130da06515","_cell_guid":"448c9843-3a64-4ad5-8287-c556b71acc3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:32.654811Z","iopub.execute_input":"2023-05-07T19:26:32.655386Z","iopub.status.idle":"2023-05-07T19:26:33.069707Z","shell.execute_reply.started":"2023-05-07T19:26:32.655347Z","shell.execute_reply":"2023-05-07T19:26:33.068268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12.4 - Previsão de amostra única no domínio de destino usando o modelo de codificador treinado no domínio de origem\n#### 12.4 - Single Sample Prediction in the Target Domain Using the Encoder Model Trained on the Source Domain","metadata":{"_uuid":"d401b672-7955-4c79-bb27-031204c73512","_cell_guid":"9bb877c3-7c97-4b59-8a07-86aba41905d1","trusted":true}},{"cell_type":"code","source":"print(x_tr[0:2].shape)\n\nx_tr_latent = encoder.predict(x_tr[0:2], batch_size=batch_size)\nx_tr_latent\n\nsvr.predict(x_tr_latent)","metadata":{"_uuid":"6ccb6acc-569b-4321-816d-7d5b15613536","_cell_guid":"b88a914f-9b93-43f9-8520-f246409f1fa1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.070983Z","iopub.execute_input":"2023-05-07T19:26:33.071331Z","iopub.status.idle":"2023-05-07T19:26:33.116985Z","shell.execute_reply.started":"2023-05-07T19:26:33.071302Z","shell.execute_reply":"2023-05-07T19:26:33.116190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12.5 - Validação de modelo: validação de dimensão\n#### 12.5 - Model Validation: Dimension Validation","metadata":{"_uuid":"cd3318d8-e297-4e8a-97f2-c4512757a0a6","_cell_guid":"72e6a6a0-aa9a-4a7f-ba52-4ed5d7cf5d3c","trusted":true}},{"cell_type":"code","source":"# No verifies the testing dataset\nprint(\"Input sinal bruto - Dimensão bruta antes de formatar:\", x_test.shape)\n\nprint(\"Dimensão após restruturação para servir com a input do autoencoder:\", \n      x_test.reshape(x_test.shape[0], \n      img_rows).shape)\n\nx_test_reshaped = x_test.reshape(x_test.shape[0], img_rows)\n\n# configura input do autoencoder (batch )\nx_input_vae = x_test_reshaped\n\nprint(\"Fazendo uma predição no VAE para obter o espaço amostral: \",\n    np.expand_dims(x_input_vae[2], axis=0).shape)\n\nx_tr_latent_pred = encoder.predict(np.expand_dims(x_input_vae[5], axis=0),\n                                   batch_size=batch_size)\n\n# x_tr_latent = encoder.predict(x_input_vae[2:3], batch_size=batch_size)\nx_tr_latent_pred\n\n# Lembrar que tenho que fazer a padronização para predição \n# (padronização do input max, minx), já configurado.\nsvr.predict(x_tr_latent_pred)","metadata":{"_uuid":"4b644cae-fb6c-4599-8ea0-4d1226c701e0","_cell_guid":"e1ab2504-75de-43a5-971e-984428e6eff0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.118282Z","iopub.execute_input":"2023-05-07T19:26:33.118612Z","iopub.status.idle":"2023-05-07T19:26:33.164657Z","shell.execute_reply.started":"2023-05-07T19:26:33.118577Z","shell.execute_reply":"2023-05-07T19:26:33.163685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13 - Modelagem de aprendizagem de transferência\n### 13 - Transfer Learning Modeling","metadata":{"_uuid":"94f0a590-4ab2-464c-ab35-b4d69a0da6a7","_cell_guid":"6931b33d-39cd-43f1-bbfb-b1790248c0d6","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport keras\nfrom keras.layers import Input\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.losses import binary_crossentropy\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"_uuid":"8f8b7dc9-30d7-42a5-b0c4-be8ef4a05c63","_cell_guid":"f2971530-33a8-490b-ae8d-60b6ef8a1c08","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.166014Z","iopub.execute_input":"2023-05-07T19:26:33.166363Z","iopub.status.idle":"2023-05-07T19:26:33.172092Z","shell.execute_reply.started":"2023-05-07T19:26:33.166327Z","shell.execute_reply":"2023-05-07T19:26:33.171209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verifiyng the shape\nprint(x_tr.shape)\nprint(y_train.shape)","metadata":{"_uuid":"8613d30f-d2b6-4343-9e63-180fcf76cd4f","_cell_guid":"6509c26c-c35e-4b8f-8ad6-d0954d42a578","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.174098Z","iopub.execute_input":"2023-05-07T19:26:33.174453Z","iopub.status.idle":"2023-05-07T19:26:33.184441Z","shell.execute_reply.started":"2023-05-07T19:26:33.174417Z","shell.execute_reply":"2023-05-07T19:26:33.183287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n### fix it\nX_train, X_test, y_train, y_test = train_test_split(x_tr, \n                                                    y_train, \n                                                    test_size=0.1,\n                                                    random_state=42)","metadata":{"_uuid":"d0a24773-5289-4252-b069-0c130515922e","_cell_guid":"3b068f6f-baab-4053-8ba0-e54c6e328e4c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.185795Z","iopub.execute_input":"2023-05-07T19:26:33.186188Z","iopub.status.idle":"2023-05-07T19:26:33.197626Z","shell.execute_reply.started":"2023-05-07T19:26:33.186152Z","shell.execute_reply":"2023-05-07T19:26:33.196765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","metadata":{"_uuid":"a77ca418-d5ff-4386-9e13-9767ffc9b478","_cell_guid":"6255b40e-a0ab-4477-a5bd-a0f95e554059","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.199135Z","iopub.execute_input":"2023-05-07T19:26:33.199566Z","iopub.status.idle":"2023-05-07T19:26:33.207868Z","shell.execute_reply.started":"2023-05-07T19:26:33.199532Z","shell.execute_reply":"2023-05-07T19:26:33.207026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test.shape)\nprint(y_test.shape)","metadata":{"_uuid":"8db43fd3-b856-455b-a3cf-15e34a717dda","_cell_guid":"dc71b931-0f69-4059-9437-ce588cdbfb73","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.209470Z","iopub.execute_input":"2023-05-07T19:26:33.210021Z","iopub.status.idle":"2023-05-07T19:26:33.219697Z","shell.execute_reply.started":"2023-05-07T19:26:33.209984Z","shell.execute_reply":"2023-05-07T19:26:33.218648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Balanced dataset\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\nover_sampler = RandomOverSampler(random_state=42)\n\nX_res, y_res = over_sampler.fit_resample(X_train, y_train)\n\nprint(f\"Training target statistics: {Counter(y_res)}\")\n# print(f\"Testing target statistics: {Counter(y_test)}\")","metadata":{"_uuid":"1f771ee0-f8d2-44d5-9cc9-055a5e57e3ec","_cell_guid":"8a42c6de-f8bb-46c2-818a-e10974d4cb4a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.221493Z","iopub.execute_input":"2023-05-07T19:26:33.221899Z","iopub.status.idle":"2023-05-07T19:26:33.235944Z","shell.execute_reply.started":"2023-05-07T19:26:33.221864Z","shell.execute_reply":"2023-05-07T19:26:33.235031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = X_res, y_res/10","metadata":{"_uuid":"9ca4022f-fd70-4e12-8fce-db0d440a27b6","_cell_guid":"fb555b16-9bbd-4f39-be75-07d292633391","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.238087Z","iopub.execute_input":"2023-05-07T19:26:33.238475Z","iopub.status.idle":"2023-05-07T19:26:33.245485Z","shell.execute_reply.started":"2023-05-07T19:26:33.238414Z","shell.execute_reply":"2023-05-07T19:26:33.244623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.1 - Carga da função de perda KCPR usada na equação MVPanP\n#### 13.1 - Loading the KCPR Loss Function Used in the MVPAnP Equation","metadata":{"_uuid":"e50e225e-be37-4bce-8a7b-454c7d989e56","_cell_guid":"ad43646a-548a-4cb3-bc40-5e77c66a4cca","trusted":true}},{"cell_type":"code","source":"# Load \nsvr_persited  = load('srv_reg.joblib') \nsvr.predict(x_tr_latent)\n\n# Load vae\nencoder_reconstruted = keras.models.load_model(\"my_model.model\")\n\nprint(x_tr[0:1].shape)\nprint(type(x_tr[0:1]))\nprint(x_tr[0:1].dtype)\n\nlatent_pred = encoder_reconstruted.predict(x_tr[0:2], batch_size=1)\nlatent_pred","metadata":{"_uuid":"a8c430b7-b404-4198-a252-39edfe232d7e","_cell_guid":"dd200bcd-42f3-46d3-a13b-71ce389ea99c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.246880Z","iopub.execute_input":"2023-05-07T19:26:33.247215Z","iopub.status.idle":"2023-05-07T19:26:33.464210Z","shell.execute_reply.started":"2023-05-07T19:26:33.247181Z","shell.execute_reply":"2023-05-07T19:26:33.463258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr.predict([[1., 1.0]])","metadata":{"_uuid":"9252a3e6-3eec-43c1-8541-2cbbb0271cb5","_cell_guid":"6267e216-9ee4-4719-b122-73c1dd125f14","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:26:33.465402Z","iopub.execute_input":"2023-05-07T19:26:33.465725Z","iopub.status.idle":"2023-05-07T19:26:33.472838Z","shell.execute_reply.started":"2023-05-07T19:26:33.465697Z","shell.execute_reply":"2023-05-07T19:26:33.471784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.2 - Implementação da função KCPR Loss (Custom Tensor Function) no modelo Eager\n#### 13.2 - Implementation of the KCPR Loss Function (Custom Tensor Function) in Eager Mode","metadata":{"_uuid":"0d50fa9a-bf05-43c3-99e8-42421d189839","_cell_guid":"a87147f0-e1cd-49f0-9d97-959792e25637","trusted":true}},{"cell_type":"code","source":"# from numpy import linalg as LA\nkl = tf.keras.losses.KLDivergence()\n\n# Reconstruting the VAE model trained on the Source Domain \nencoder_reconstruted = keras.models.load_model(\"my_model.model\")  \n\ndef KPCR_Prediction_Loss(input_tensor):\n        \n    z_latent = tensorflow_graph(np.expand_dims(input_tensor, axis=0),\n                                False, None).numpy()\n        \n    src_pred = svr.predict(z_latent)\n          \n    return src_pred # np.sinh(input_tensor)\n\n@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\ndef KPCR_loss(input):\n    \n    \n  y = tf.numpy_function(KPCR_Prediction_Loss, [input], tf.float32).numpy()\n\n  return y\n\n'''\ndef KPCR_loss(input_tensor):\n    \n    latent_pred = encoder_reconstruted.predict(\n                                               input_tensor.numpy(), \n                                               batch_size=1)\n\n    return 0 #svr_persited.predict(latent_pred) \n'''\n\ndef MVPAnP_Loss(data, y_pred):\n    \n    y_true = data[0][0]\n    input2latent = data[0][1:1601]\n\n    return K.mean(K.square(y_pred - y_true),\n                  axis=-1) + 0.1* kl(KPCR_loss(input2latent),\n                                     y_true)\n\n# MVPAnP_Loss --> Loss function sum of components\n# MVPAnP_Loss --> custom_loss2\n# my_numpy_func --> KPCR_Prediction_Loss\n# tf_function(data) # KPCR_loss(data )","metadata":{"_uuid":"4e484ec6-adb7-4162-8ee3-7a4c3df7edcb","_cell_guid":"ebfac909-2720-44ac-8cce-5dce361d38b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:29:12.687538Z","iopub.execute_input":"2023-05-07T19:29:12.687881Z","iopub.status.idle":"2023-05-07T19:29:12.844134Z","shell.execute_reply.started":"2023-05-07T19:29:12.687851Z","shell.execute_reply":"2023-05-07T19:29:12.843339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.3 - Criando um modelo simples de MLP (Multilayer Perceptron) para testar a função de perda personalizada\n#### 13.3 - Creating a Simple MLP (Multilayer Perceptron) Model to Test the Custom Loss Function","metadata":{"_uuid":"7d60bdab-79a8-46cb-ad77-e5d6fddd4890","_cell_guid":"8b556613-4b1c-42ca-950c-59ba94d7c706","trusted":true}},{"cell_type":"code","source":"# create model\ntf.config.run_functions_eagerly(True)\n\ni = Input(shape=(1600,))\nx = Dense(50, kernel_initializer='glorot_uniform', activation='relu')(i)\nx = Dense(50, activation='relu')(x)\nx = Dense(25, activation='sigmoid')(x)\no = Dense(1, activation='sigmoid')(x)\n#o = Dense(1, kernel_initializer='normal', activation='linear')(x)\nmodel = Model(i, o)\n\nmodel.compile(loss=MVPAnP_Loss, optimizer='adam') \n\nhistory = model.fit(X_train, np.column_stack((y_train, X_train)), \n                    epochs=5, \n                    batch_size=1,\n                    verbose=0)\n\n#model.fit(X_train, [X_train,y_train], epochs=1, batch_size=1,verbose=1)","metadata":{"_uuid":"41a97da9-505e-4e01-90b8-ca2cdd6981e4","_cell_guid":"c41148e1-47b2-42f3-b2ce-2dda38c52cac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:29:12.847935Z","iopub.execute_input":"2023-05-07T19:29:12.848203Z","iopub.status.idle":"2023-05-07T19:30:06.411731Z","shell.execute_reply.started":"2023-05-07T19:29:12.848177Z","shell.execute_reply":"2023-05-07T19:30:06.410811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X_train)","metadata":{"_uuid":"a10a0f20-d224-434c-964c-cd0b35eb2bbf","_cell_guid":"7d4ad47c-4a41-4cc8-b4b0-63b5267d1f73","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.413200Z","iopub.execute_input":"2023-05-07T19:30:06.413551Z","iopub.status.idle":"2023-05-07T19:30:06.419160Z","shell.execute_reply.started":"2023-05-07T19:30:06.413513Z","shell.execute_reply":"2023-05-07T19:30:06.418320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","metadata":{"_uuid":"c93302fb-3e4f-4745-a64c-7cf5130b9d84","_cell_guid":"508034ed-b5c1-43f7-8318-ced32ab2a37e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.420686Z","iopub.execute_input":"2023-05-07T19:30:06.421306Z","iopub.status.idle":"2023-05-07T19:30:06.431432Z","shell.execute_reply.started":"2023-05-07T19:30:06.421270Z","shell.execute_reply":"2023-05-07T19:30:06.430679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.column_stack((y_train, X_train))\ndata.shape","metadata":{"_uuid":"cab77207-b980-4dde-ac77-c8fc5d6ffc6f","_cell_guid":"325d46ba-8fce-401f-bf65-7a87b0faaea6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.434899Z","iopub.execute_input":"2023-05-07T19:30:06.435256Z","iopub.status.idle":"2023-05-07T19:30:06.449057Z","shell.execute_reply.started":"2023-05-07T19:30:06.435210Z","shell.execute_reply":"2023-05-07T19:30:06.448111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nhistory = model.fit(X_train, \n                    [X_train,y_train], \n                    epochs=1, \n                    batch_size=1,\n                    verbose=0)\n\"\"\"\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"_uuid":"a7dc189e-da3e-48c0-a062-fe7525bc8fb9","_cell_guid":"9a622059-901a-4a48-89c8-31fcb1f9d4fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.452542Z","iopub.execute_input":"2023-05-07T19:30:06.452803Z","iopub.status.idle":"2023-05-07T19:30:06.606235Z","shell.execute_reply.started":"2023-05-07T19:30:06.452776Z","shell.execute_reply":"2023-05-07T19:30:06.605347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evalute model\nscore = model.evaluate(X_train, np.column_stack((y_train, X_train)), verbose=1)\nprint('Test loss:', score)","metadata":{"_uuid":"ab005d74-1282-4f09-aa94-842b3ae8b44f","_cell_guid":"0f442bec-05c2-4579-9398-7e24dcb9e441","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.607662Z","iopub.execute_input":"2023-05-07T19:30:06.608004Z","iopub.status.idle":"2023-05-07T19:30:06.918451Z","shell.execute_reply.started":"2023-05-07T19:30:06.607967Z","shell.execute_reply":"2023-05-07T19:30:06.917738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a prediction\nyhat = model.predict(X_train)\nyhat.shape","metadata":{"_uuid":"1232a7ee-3607-43aa-9160-a946cd5af6f5","_cell_guid":"ee1f42d2-4861-4b7e-92b1-b155f6db1e72","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:06.921496Z","iopub.execute_input":"2023-05-07T19:30:06.921763Z","iopub.status.idle":"2023-05-07T19:30:07.011603Z","shell.execute_reply.started":"2023-05-07T19:30:06.921736Z","shell.execute_reply":"2023-05-07T19:30:07.010575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"_uuid":"e330de11-e37f-4121-b02d-4514baf44c19","_cell_guid":"4536a959-7b9e-4218-a34e-916ff3fa0777","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:07.012858Z","iopub.execute_input":"2023-05-07T19:30:07.013196Z","iopub.status.idle":"2023-05-07T19:30:07.027913Z","shell.execute_reply.started":"2023-05-07T19:30:07.013168Z","shell.execute_reply":"2023-05-07T19:30:07.027208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.4 - Validação do modelo MVPanP: usando otimização de hiperparâmetros e ANOVA\n#### 13.4 - MVPAnP Model Validation: Using Hyperparameter Optimization and ANOVA","metadata":{"_uuid":"6a4aae74-d0af-4726-ab81-ea5332b74933","_cell_guid":"1a30690c-de01-4fe6-9888-760d0e2acb6b","trusted":true}},{"cell_type":"code","source":"\"\"\"\nOptuna example that optimizes a neural network \nclassifier configuration for the\nMNIST dataset using Keras.\nIn this example, we optimize the validation accuracy of \nMNIST classification using\nKeras. We optimize the filter and kernel size, \nkernel stride and layer activation.\n# reference: \nhttps://github.com/optuna/optuna-examples/blob/main/keras/keras_simple.py\n\"\"\"\nimport urllib\nimport warnings\n\nimport optuna\n\nfrom keras.backend import clear_session\nfrom keras.datasets import mnist\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop","metadata":{"_uuid":"9cb6a7d5-3857-4a4e-a03b-9d39d4b4688c","_cell_guid":"b0d38754-015c-4ac1-9a01-fa5871c1d617","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:07.029235Z","iopub.execute_input":"2023-05-07T19:30:07.029628Z","iopub.status.idle":"2023-05-07T19:30:07.035158Z","shell.execute_reply.started":"2023-05-07T19:30:07.029563Z","shell.execute_reply":"2023-05-07T19:30:07.034251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.4.1 - Otimização de Hiperparâmetros para Modelo de Linha de Base (Rede Neural Convolucional 1D) sem Transfer Learning\n##### 13.4.1 - Hyperparameter Optimization for Baseline Model (1D Convolutional Neural Network) without Transfer Learning","metadata":{"_uuid":"e4320cb0-3dbb-4ac8-a1f5-6ac86791a44d","_cell_guid":"91078074-e16c-488f-8c7c-285d6e1cfc72","trusted":true}},{"cell_type":"code","source":"def start_Baseline(features1,features2, trials,  plot_graph = False):\n\n  # 1D CONV NET MODEL\n  def create_model(activation, \n                   dropout_rate, \n                   filters, \n                   kernel_size, \n                   strides, \n                   kernel_initializer):\n    \n    \"\"\"\n    Create an temporary model\n    \"\"\"\n    clear_session()\n    \n    '''\n    # 1D CNN neural network - 1D CNN model starts\n    model_m.add(Reshape((TIME_PERIODS, num_sensors),\n                         input_shape=(input_shape,)))\n                         \n    model_m.add(Conv1D(100, 10, activation='relu', \n                 input_shape=(TIME_PERIODS, \n                 num_sensors)))\n                 \n    i = Reshape((1600, 1), input_shape=(1600,))\n    \n    filters = num_filters[0]\n    print(\"FILTER VALUES ----------\" , filters)\n    '''\n\n    if type(filters) == tuple:\n        filter_num = filters[0]\n    else:\n        filter_num = filters\n        \n    print(\"FILTER VALUES ----------\" , filters)    \n    print(\"FILTER VALUES ----------\" , filter_num)\n    \n    i = Input(shape=(1600,1))\n    #x = Conv1D(100, 10, kernel_initializer=kernel_initializer, \n    #activation=activation, input_shape=(1600, 1))(i)\n    x = Conv1D(filters=filter_num, \n               kernel_size=kernel_size, \n               strides=strides, \n               kernel_initializer=kernel_initializer, \n               activation=activation,\n               input_shape=(1600, 1))(i)\n    #i = Input(shape=(1600,))\n    x = Conv1D(100, 10, activation='relu')(x)\n    x = Conv1D(100, 10, activation='relu')(x)\n    x = MaxPooling1D(3)(x)\n    x = Conv1D(160, 10, activation='relu')(x)\n    x = Conv1D(160, 10, activation='relu')(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(rate=dropout_rate)(x)\n    x = Dense(40, activation='relu')(x)\n    o = Dense(1, activation='sigmoid')(x)\n    \n    ### - 1D CNN model ends\n    '''\n    i = Input(shape=(1600,))\n    x = Dense(50, kernel_initializer=kernel_initializer, \n                  activation=activation)(i)\n    x = Dense(50, activation='relu')(x)\n    x = Dense(25, activation='sigmoid')(x)\n    o = Dense(1, activation='sigmoid')(x)\n    '''\n    model = Model(i, o)\n\n    #model.compile(loss=MVPAnP_Loss, optimizer='adam') \n    model.compile(loss='mean_squared_error', optimizer='adam')\n\n    return model\n\n  # Objective function to optimize by OPTUNA\n  def objective(trial):\n    activation = trial.suggest_categorical(\"activation\", \n                                           [\"relu\", \"sigmoid\", \"swish\"]\n                                          )\n    dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.3)\n    num_filters = trial.suggest_categorical(\"filters\", [32, 64]),\n    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5]),\n    strides = trial.suggest_categorical(\"strides\", [1, 2]),\n            \n    if (activation == \"relu\"):\n      model = create_model(activation,\n                           dropout_rate=dropout_rate,\n                           filters=num_filters,\n                           kernel_size=kernel_size, \n                           strides=strides,\n                           kernel_initializer=\"HeUniform\")\n    else:\n      model = create_model(activation, \n                           dropout_rate=dropout_rate,\n                           filters=num_filters, \n                           kernel_size=kernel_size, \n                           strides=strides,\n                           kernel_initializer=\"GlorotUniform\",)\n        \n    # Implement early stopping criterion. \n    # Training process stops when there is \n    # no improvement during 50 iterations\n    callback = keras.callbacks.EarlyStopping(monitor='loss',\n                                             patience=50\n                                            )\n    history = model.fit(features1, \n                        features2,\n                        batch_size = 1,\n                        epochs=3,\n                        callbacks = [callback], \n                        verbose = 0)\n    \n    return history.history[\"loss\"][-1]\n  \n  study = optuna.create_study(direction='minimize')\n  study.optimize(objective, n_trials=trials)\n    \n  # Create final model with the best hyperparams\n  print('Best hyperparams found by Optuna: \\n', study.best_params)\n  if (study.best_params['activation'] == \"relu\"):\n    \n    model = create_model(study.best_params['activation'], \n                         study.best_params['dropout'], \n                         study.best_params['filters'], \n                         study.best_params['kernel_size'],\n                         study.best_params['strides'],\n                         kernel_initializer=\"HeUniform\",\n                        )\n  else:\n    model = create_model(study.best_params['activation'], \n                         study.best_params['dropout'], \n                         study.best_params['filters'], \n                         study.best_params['kernel_size'],\n                         study.best_params['strides'],\n                         kernel_initializer=\"GlorotUniform\",)\n\n  # model.fit(X_train, np.column_stack((y_train, X_train)), \n  # epochs=5, batch_size=1,verbose=0)\n  # model.compile(optimizer='rmsprop')\n\n  model.summary()\n    \n  # Implement early stopping criterion. \n  # Training process stops when there is no improvement during 50 iterations\n  callback = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n  history = model.fit(features1, \n                      features2,\n                      batch_size = 1,\n                      epochs=2, \n                      callbacks = [callback],\n                      verbose = 0)\n\n  result = model.predict(features1)\n    \n  # Result evaluation\n  print(f'RMSE Autoencoder: {np.sqrt(mean_squared_error(features2, result))}')\n  print('')\n\n  # Following values are returned: extracted_f || MSE || OPTUNA best hyperparams\n\n  return mean_squared_error(features2, result), study.best_params, study\n\nAcoder_MSE, Acoder_hyperparams, Study = start_Baseline(X_train, y_train,\n                                  trials = 10,\n                                  plot_graph=True)","metadata":{"_uuid":"e6a7bf9a-67f2-41a5-8682-c209eef41622","_cell_guid":"cdcca69c-53c6-46c1-a77e-a8b47a64ebb4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:30:07.037314Z","iopub.execute_input":"2023-05-07T19:30:07.037712Z","iopub.status.idle":"2023-05-07T19:36:46.297622Z","shell.execute_reply.started":"2023-05-07T19:30:07.037649Z","shell.execute_reply":"2023-05-07T19:36:46.295846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save results\ndf_results_without_TF = Study.trials_dataframe()\n\n# df_results.to_pickle(results_directory + 'df_optuna_results.pkl')\n# df_results.to_csv(results_directory + 'df_optuna_results.csv')\ndf_results_without_TF","metadata":{"_uuid":"2c5ba1f8-2996-43fb-a19f-3241eb5e94bb","_cell_guid":"bbf58cce-33b9-4e88-9e97-e5299c2f978f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:36:46.300760Z","iopub.execute_input":"2023-05-07T19:36:46.301159Z","iopub.status.idle":"2023-05-07T19:36:46.333426Z","shell.execute_reply.started":"2023-05-07T19:36:46.301127Z","shell.execute_reply":"2023-05-07T19:36:46.332576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(Study.trials)))\n\nprint(\"Best trial:\")\ntrial = Study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"_uuid":"84f7b3af-0892-4b3a-9092-65609d809484","_cell_guid":"cab5dcb7-957d-465b-99cb-24f10f9dde0d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:36:46.335060Z","iopub.execute_input":"2023-05-07T19:36:46.335594Z","iopub.status.idle":"2023-05-07T19:36:46.346444Z","shell.execute_reply.started":"2023-05-07T19:36:46.335557Z","shell.execute_reply":"2023-05-07T19:36:46.345210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.4.2 - Otimização de hiperparâmetros para modelo de linha de base (1D CONVNET) - COM Transfer Learning\n### 13.4.2 - Hyperparameter Optimization for Baseline Model (1D CONVNET) - WITH Transfer Learning","metadata":{"_uuid":"aa00cef8-5655-4069-a9c4-a2433e773fa9","_cell_guid":"5b965bf5-4a42-4342-adc6-4fbfa3379f39","trusted":true}},{"cell_type":"code","source":"def start_Benchmarking(features1,features2, trials,  plot_graph = False):\n\n  # 1D CONV NET MODEL\n  def create_model(activation,dropout_rate, \n                   filters, kernel_size, \n                   strides, kernel_initializer):\n        \n    clear_session()\n    #from numpy import linalg as LA\n    kl = tf.keras.losses.KLDivergence()\n    encoder_reconstruted = keras.models.load_model(\"my_model.model\")  \n\n    def KPCR_Prediction_Loss(input_tensor):\n\n        z_latent = tensorflow_graph(np.expand_dims(input_tensor, axis=0),\n                                    False, None).numpy()\n\n        src_pred = svr.predict(z_latent)\n\n        return src_pred # np.sinh(input_tensor)\n\n    @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\n    def KPCR_loss(input):\n\n      y = tf.numpy_function(KPCR_Prediction_Loss, [input], tf.float32).numpy()\n\n      return y\n\n    '''\n    def KPCR_loss(input_tensor):\n\n        latent_pred = encoder_reconstruted.predict(input_tensor.numpy(), batch_size=1)\n\n        return 0 #svr_persited.predict(latent_pred) \n    '''\n\n    def MVPAnP_Loss(data, y_pred):\n        y_true = data[0][0]\n        input2latent = data[0][1:1601]\n        # MSE loss function\n\n        return K.mean(K.square(y_pred - y_true), axis=-1) - 0.1*kl(KPCR_loss(input2latent), y_true)  \n    \n    # tf_function(data) # KPCR_loss(data )\n    # create model\n    \n    tf.config.run_functions_eagerly(True)\n\n    # Bug fix\n    if type(filters) == tuple:\n        filter_num = filters[0]\n    else:\n        filter_num = filters\n        \n    print(\"FILTER VALUES ----------\" , filters)    \n    print(\"FILTER VALUES ----------\" , filter_num)\n    \n    i = Input(shape=(1600,1))\n    x = Conv1D(filters=filter_num, kernel_size=kernel_size, strides=strides, \n               kernel_initializer=kernel_initializer, \n               activation=activation, input_shape=(1600, 1))(i)\n    x = Conv1D(100, 10, activation='relu')(x)\n    x = Conv1D(100, 10, activation='relu')(x)\n    x = MaxPooling1D(3)(x)\n    x = Conv1D(160, 10, activation='relu')(x)\n    x = Conv1D(160, 10, activation='relu')(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(rate=dropout_rate)(x)\n    x = Dense(40, activation='relu')(x)\n    o = Dense(1, activation='sigmoid')(x)\n    model = Model(i, o)\n\n    model.compile(loss=MVPAnP_Loss, optimizer='adam') \n\n    return model\n\n  # Objective function to optimize by OPTUNA\n  def objective(trial):\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\", \"swish\"])\n    dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.3)\n    num_filters = trial.suggest_categorical(\"filters\", [32, 64]),\n    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 5]),\n    strides = trial.suggest_categorical(\"strides\", [1, 2])\n    \n    if (activation == \"relu\"):\n      model = create_model(activation, \n                           dropout_rate=dropout_rate,\n                           filters=num_filters,\n                           kernel_size=kernel_size, \n                           strides=strides,\n                           kernel_initializer=\"HeUniform\")\n    else:\n      model = create_model(activation,\n                           dropout_rate=dropout_rate,\n                           filters=num_filters, \n                           kernel_size=kernel_size, \n                           strides=strides,\n                           kernel_initializer=\"GlorotUniform\")\n    \n    # Implement early stopping criterion. \n    # Training process stops when there is no improvement during 50 iterations\n    \n    callback = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n    history = model.fit(features1, \n                        features2,\n                        batch_size = 1,\n                        epochs=3,\n                        callbacks = [callback], \n                        verbose = 0)\n    \n    return history.history[\"loss\"][-1]\n  \n  study = optuna.create_study(direction='minimize')\n  study.optimize(objective, n_trials=trials)\n    \n  # Create final model with the best hyperparams\n  print('Best hyperparams found by Optuna: \\n', study.best_params)\n  if (study.best_params['activation'] == \"relu\"):\n    model = create_model(study.best_params['activation'],\n                         study.best_params['dropout'], \n                         study.best_params['filters'], \n                         study.best_params['kernel_size'],\n                         study.best_params['strides'],\n                         kernel_initializer=\"HeUniform\",\n                        )\n  else:\n    model = create_model(study.best_params['activation'], \n                         study.best_params['dropout'], \n                        study.best_params['filters'], \n                         study.best_params['kernel_size'],\n                         study.best_params['strides'],\n                         kernel_initializer=\"GlorotUniform\")\n\n  model.summary()\n  # Implement early stopping criterion. \n  # Training process stops when there is no improvement during 50 iterations\n  callback = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n  history = model.fit(features1, \n                      features2,\n                      batch_size = 1,\n                      epochs=2, \n                      callbacks = [callback],\n                      verbose = 0)\n\n  result = model.predict(features1)\n    \n  # Result evaluation\n  print(f'RMSE Autoencoder: {np.sqrt(mean_squared_error(features2[:,0], result))}')\n\n  # Following values are returned: extracted_f || MSE || OPTUNA best hyperparams\n\n  return mean_squared_error(features2[:,0], result), study.best_params, study\n\nAcoder_MSE, Acoder_hyperparams, Study = start_Benchmarking(X_train, \n                                                           np.column_stack((y_train,\n                                                                            X_train)),\n                                  trials = 10,\n                                  plot_graph=True)","metadata":{"_uuid":"c2e94124-840a-4c58-9f76-85fed4453592","_cell_guid":"c732dfa0-8d25-4a94-8f43-2a0a0953a393","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:36:46.348687Z","iopub.execute_input":"2023-05-07T19:36:46.349127Z","iopub.status.idle":"2023-05-07T19:45:40.487818Z","shell.execute_reply.started":"2023-05-07T19:36:46.349083Z","shell.execute_reply":"2023-05-07T19:45:40.486989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results_with_TF = Study.trials_dataframe()\n#df_results.to_pickle(results_directory + 'df_optuna_results.pkl')\n#df_results.to_csv(results_directory + 'df_optuna_results.csv')\ndf_results_with_TF","metadata":{"_uuid":"877a6d53-2d8d-4f7a-a312-ebe9e0cf8e96","_cell_guid":"3e71bedc-8bc6-4844-a9f3-23c5edc606c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.489424Z","iopub.execute_input":"2023-05-07T19:45:40.489729Z","iopub.status.idle":"2023-05-07T19:45:40.512980Z","shell.execute_reply.started":"2023-05-07T19:45:40.489698Z","shell.execute_reply":"2023-05-07T19:45:40.511913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results_with_TF[\"value\"].values","metadata":{"_uuid":"7b2a15fa-f864-4700-8533-f959480f97b3","_cell_guid":"fa634361-c4b1-4977-aea1-64db442e119d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.514641Z","iopub.execute_input":"2023-05-07T19:45:40.515390Z","iopub.status.idle":"2023-05-07T19:45:40.530649Z","shell.execute_reply.started":"2023-05-07T19:45:40.515344Z","shell.execute_reply":"2023-05-07T19:45:40.529544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(Study.trials)))\n\nprint(\"Best trial:\")\ntrial = Study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"_uuid":"2915aafd-de0a-4796-a0f0-10c89036a109","_cell_guid":"db1bf0c8-7e6a-42a3-a629-3035bc8ebdf0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.532586Z","iopub.execute_input":"2023-05-07T19:45:40.532999Z","iopub.status.idle":"2023-05-07T19:45:40.545064Z","shell.execute_reply.started":"2023-05-07T19:45:40.532958Z","shell.execute_reply":"2023-05-07T19:45:40.543796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13.5 - Análise ANOVA para o modelo de linha de base antes e depois do TF\n#### 13.5 - ANOVA analysis for the baseline model before and after TF","metadata":{"_uuid":"f672d664-9cf8-4e06-84a3-431617dd2e68","_cell_guid":"e4b6971d-ce7d-4af3-a2c4-38b648a36a6e","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nfrom scipy import stats\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nprint('Libraries imported')","metadata":{"_uuid":"5e378ac4-f402-46ce-8bd2-31ab4d222d52","_cell_guid":"bb7394cb-d01c-4c32-8d36-2d4362db1a93","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.547022Z","iopub.execute_input":"2023-05-07T19:45:40.547536Z","iopub.status.idle":"2023-05-07T19:45:40.659160Z","shell.execute_reply.started":"2023-05-07T19:45:40.547496Z","shell.execute_reply":"2023-05-07T19:45:40.658300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results_with_TF[\"Transfer_Learning\"] = \"Com TF\"\ndf_results_without_TF[\"Transfer_Learning\"] = \"Sem TF\"","metadata":{"_uuid":"bd2a84e7-b8f8-4c7e-9fca-64ed2a8522b6","_cell_guid":"de257d23-b0ad-489e-b692-0afb961b7c34","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.660288Z","iopub.execute_input":"2023-05-07T19:45:40.660598Z","iopub.status.idle":"2023-05-07T19:45:40.666224Z","shell.execute_reply.started":"2023-05-07T19:45:40.660571Z","shell.execute_reply":"2023-05-07T19:45:40.665362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge pandsa dataframe\nbenchmarking_analysis = pd.concat([df_results_without_TF, df_results_with_TF])\n\n# Formated the decimal places\nbenchmarking_analysis['value'] = benchmarking_analysis['value'].values.round(4)","metadata":{"_uuid":"c9685722-28b5-44ca-9b52-997f10a0915a","_cell_guid":"12cb81bf-a45c-4c34-a7a5-9e1a3eec013d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.667444Z","iopub.execute_input":"2023-05-07T19:45:40.667939Z","iopub.status.idle":"2023-05-07T19:45:40.685417Z","shell.execute_reply.started":"2023-05-07T19:45:40.667905Z","shell.execute_reply":"2023-05-07T19:45:40.684632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmarking_analysis['value'].values.round(4)","metadata":{"_uuid":"34d4e38d-9aeb-48b4-b4ab-d623f8317a67","_cell_guid":"8adae99b-e2c0-4ad2-a236-00304202146b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.688556Z","iopub.execute_input":"2023-05-07T19:45:40.688898Z","iopub.status.idle":"2023-05-07T19:45:40.698215Z","shell.execute_reply.started":"2023-05-07T19:45:40.688872Z","shell.execute_reply":"2023-05-07T19:45:40.696675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14 - Comparação dos resultados dos métodos propostos de TF\n## 14 - Comparison of the results of the proposed methods of TF","metadata":{"_uuid":"40de8ac5-8b4f-4eee-b1e3-892e38d1bbbb","_cell_guid":"b58128b0-f3b1-46ab-80c7-bef3f065cd23","trusted":true}},{"cell_type":"code","source":"benchmarking_analysis","metadata":{"_uuid":"341fd9de-dd1b-448c-89e6-d82cf5d22607","_cell_guid":"ff265c1c-5d7a-4dbb-8ef3-de8f2e2374dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.699890Z","iopub.execute_input":"2023-05-07T19:45:40.700540Z","iopub.status.idle":"2023-05-07T19:45:40.723409Z","shell.execute_reply.started":"2023-05-07T19:45:40.700499Z","shell.execute_reply":"2023-05-07T19:45:40.722446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx = benchmarking_analysis[benchmarking_analysis['Transfer_Learning'] == 'Sem TF']['value'].values\nxy = benchmarking_analysis[benchmarking_analysis['Transfer_Learning'] == 'Com TF']['value'].values\n# Importing library\nfrom scipy.stats import f_oneway\n# Conduct the one-way ANOVA\nf_oneway(xx, xy)","metadata":{"_uuid":"f701a30a-f6ba-4615-babc-5cab04247380","_cell_guid":"0ef35136-0979-4a55-907f-882a43043d39","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.724759Z","iopub.execute_input":"2023-05-07T19:45:40.725152Z","iopub.status.idle":"2023-05-07T19:45:40.740275Z","shell.execute_reply.started":"2023-05-07T19:45:40.725112Z","shell.execute_reply":"2023-05-07T19:45:40.739268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 14.1 - Análise ANOVA e boxplot\n#### 14.1 - Anova Analysis and BoxPlot","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Creating dataset\nnp.random.seed(10)\n\ndata = [xx, xy]\nplt.rcParams['font.size'] = '16'\nfig = plt.figure(figsize =(10, 5))\n\n# Creating axes instance\nax = fig.add_axes([0, 0, 1, 1])\n\n# Creating plot\nbp = ax.boxplot(data)\n\n# x-axis labels\nax.set_xticklabels(['Sem Transferência', 'Com transferência'])\n#ax.set_yticklabels(\"Means Square erro\")\nax.set_ylabel(\"Means Square Error\")\n\n# show plot\nplt.show()","metadata":{"_uuid":"3a4878b1-063f-47c2-8e4a-ee9a98dc9b91","_cell_guid":"d8148322-6886-4896-8c8d-1acb3b7f06c0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.743864Z","iopub.execute_input":"2023-05-07T19:45:40.744117Z","iopub.status.idle":"2023-05-07T19:45:40.900701Z","shell.execute_reply.started":"2023-05-07T19:45:40.744086Z","shell.execute_reply":"2023-05-07T19:45:40.899825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=benchmarking_analysis['value'], groups=benchmarking_analysis ['Transfer_Learning'], alpha=0.05)\nprint(tukey)","metadata":{"_uuid":"6e88f477-59c9-4958-9a84-67297dc32f13","_cell_guid":"6b56d7f9-f9fe-4249-ba94-90ed7a50ccdf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-07T19:45:40.901983Z","iopub.execute_input":"2023-05-07T19:45:40.902519Z","iopub.status.idle":"2023-05-07T19:45:40.923148Z","shell.execute_reply.started":"2023-05-07T19:45:40.902478Z","shell.execute_reply":"2023-05-07T19:45:40.922185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}